# Multimodal Chain-of-Thought Reasoning in Language Models

날짜: 2025년 2월 13일

- [https://arxiv.org/pdf/2302.00923](https://arxiv.org/pdf/2302.00923)
- TMLR 2024(2023)
    - 요즘 인정받는 저널
    - Neurips, icml ,iclr, ( aistats, colt, uai )~> tmlr >> aaai >>ijcai 라는 커뮤니티 글을 본 적이 있음
    - Openreview 씀
- MemeMQA 에서 하도 언급돼서 읽어보려 함
    - 엄청 유명한 논문이고 인용수 423회
- CoT 방법론에 대해 먼저 검증, 분석해보고 해결책을 찾아가는 방식으로 서술되어 있는데
    - 해결책을 갑자기 멀티모달(비전)을 사용하는걸로 함
    - 근거는 제시되어 있는데 CoT 두 단계 프레임워크의 완전한 해결책?이라고 보기엔 음..
    - 예전에 대회 나갔을 때 저런 방식으로 CoT 적용하면 성능 오히려 내려갔었는데 그게 실험 결과로 나와있으니 반갑기도..? 하고 진짜네 싶기도 함
    - 요즘 대규모 멀티모달 LLM이 워낙 많이 나와서 또 다르게 One stage 쓰는게 더 적합할 수도 있겠다는 생각..

## Abstract

- 사고의 연쇄(chain-of-thought, CoT) 추론: 복잡한 추론에서 인상적인 성능
- 기존 CoT 연구: 주로 언어 모달리티에 중점
- 본 연구: 언어(텍스트)와 비전(이미지) 모달리티를 통합, 근거 생성과 답변 유추를 분리하는 두 단계 프레임워크 멀티모달-CoT를 제안
    - ScienceQA와 A-OKVQA 벤치마크 데이터셋에 대한 실험 결과는 우리가 제안한 접근 방식의 효과
    - 멀티모달-CoT가 환각을 완화하고 수렴 속도를 향상시키는 이점을 제공

## 1. Introduction

- 기존 CoT 추론과 관련된 연구: 대체로 언어 모달리티에 국한
- 본 연구: Multimodal-CoT 패러다임 제안
    - 다단계 문제를 중간 추론 단계(이유)로 분해 → 답 추론
    - 두 가지 모달리티에 중점: 비전과 언어

- Multimodal-CoT 추론은 두 가지 주요 패러다임을 통해
    - (i) 대규모 언어 모델(LLM)을 프롬프트
        - 서로 다른 모달리티의 입력을 통합 모달리티로 변환
        - LLM에게 CoT를 수행하도록 프롬프트하는 것
        - 이미지에 대한 캡션 생성 → 원래 텍스트와 결합하여 LLM에 입력
        - 여전히 비전 신호를 텍스트 설명으로 변환할 때 상당한 정보 손실을 초래
    - (ii) 작은 모델을 파인튜닝
        - 모델 아키텍처를 조정할 유연성을 제공
        - hallucination 경향 있음
            - hallucination를 완화하기 위해 언어(텍스트)와 비전(이미지) 모달리티를 통합한 멀티모달-CoT를 제안
            - 생성을 분리하여 답변 추론이 이루어지는 두 단계 프레임워크

- 기여점
    - 다양한 모달리티의 CoT 추론에 대한 첫 번째 연구
    - 언어 모델을 파인튜닝하여 비전과 언어 표현을 융합하는 두 단계 프레임워크를 제안
    - 나이브한 방법으로 CoT를 사용하는 것이 실패하는 이유와 비전 기능을 통합

`

## 2. Background

### 2.1 LLM과 함께하는 CoT 추론

- 최근 CoT는 LLM의 다단계 추론 능력을 이끌어내기 위해 널리 사용 중
    - CoT 기법은 LLM이 문제를 해결하기 위한 중간 추론 체인을 생성하도록 장려
    - 두 가지 주요 기법: 제로샷(CoT)과 소수샷(CoT)으로 CoT 추론을 수행
    - 제로샷(CoT): “단계별로 생각해 보자”라는 프롬프트를 테스트 질문 뒤에 추가해 CoT 추론을 유도
    - 소수샷(CoT): 몇 가지 단계별 추론 시연이 추론 조건으로 사용
        - 각 시연에는 질문과 최종 답변으로 이어지는 추론 체인이 포함
        - 일반적으로 수작업으로 제작되거나 자동 생성
        - 수작업 제작과 자동 생성을 각각 수동(CoT)과 자동(CoT)으로 지칭
    - 최근 대부분의 연구는 소수샷(CoT)을 개선하는 데 초점 → 두 가지 주요 연구 방향으로 분류:
    - → (i) 시연 최적화; (ii) 추론 체인 최적화

- **Optimizing Demonstrations**
    - demonstrations: 시연, LLM이 문제를 풀 때 참고할 수 있도록 제공되는 **질문과 이에 대한 단계별 추론 과정 및 정답의 예제들**
    - 소수샷(CoT)의 성능은 시연의 질에 따라 달라짐
        - 서로 다른 주석자가 작성한 시연을 사용할 경우 추론 작업에서 정확도가 크게 차이
    - 시연을 수작업으로 제작하는 것을 넘어, 최근 연구는 시연 선택 과정 최적화 방법을 조사
        - 주어진 데이터셋의 질문을 몇 개의 클러스터로 나누고
        - 각 클러스터에서 대표적인 질문을 샘플링하여
        - 제로샷(CoT)와 단순한 휴리스틱을 사용하여 그 추론 체인을 생성하는 방법을 제안
    - 효과적인 시연을 얻기 위해 강화 학습(RL) 및 복잡성 기반 선택 전략이 제안
        - Fu et al. (2022): 복잡한 추론 체인(즉, 더 많은 추론 단계를 가진) 예제를 시연 선택
        - Lu et al. (2022b): 후보 풀에서 최적의 맥락 내 예제를 찾아내고 GPT-3.5와 상호작용하면서 주어진 훈련 예제에서 예측 보상을 극대화하는 에이전트를 훈련

- **Optimizing Reasoning Chains**
    - 추론 체인 최적화 방법: 문제 분해
        - Zhou et al. (2022): 복잡한 문제를 하위 문제로 분해한 다음 이러한 하위 문제를 순차적으로 해결하는 최소에서 최대 프롬프트를 제안
        - Khot et al. (2022): 다양한 분해 구조를 사용하고 각 하위 질문에 대한 답변을 위해 서로 다른 프롬프트를 설계
    - Program-of-Thoughts
        - Chen et al. (2022): 사고 프로그램(Program-of-Thoughts, PoT)을 제안하여 추론 과정을 프로그램으로 모델링, LLM이 생성된 프로그램을 실행하여 답변을 도출하도록 유도
    - 테스트 질문에 대해 여러 추론 경로를 투표하는 것
        - Wang et al. (2022b): LLM의 여러 출력을 샘플링한 다음 최종 답변에 대해 다수결을 적용하는 자기 일관성 디코딩 전략을 도입
        - Wang et al. (2022c)와 Li et al. (2022c): 입력 공간에 무작위성을 도입하여 투표를 위해 보다 다양한 출력을 생성

### 2.2 Eliciting CoT Reasoning by Fine-Tuning Models

- **언어 모델 미세 조정을 통해 CoT 추론 생성**
    - Lu et al. (2022a):
        - CoT 주석이 있는 대규모 데이터셋에서 인코더-디코더 T5 모델을 미세 조정
        - 그러나, CoT를 사용하여 답변을 유추할 때 급격한 성능 저하
        - 답변(추론) 이전에 추론 체인을 생성, CoT는 답변 이후에 설명으로만 사용
    - Magister et al. (2022)와 Ho et al. (2022): 큰 교사 모델이 생성한 사고의 연쇄(output)를 기반으로 학생 모델을 미세 조정하여 지식 증류를 적용
    - Wang et al. (2022a): 현재 단계의 맥락에 조건화된 프롬프트를 동적으로 합성하는 반복적인 맥락 인식 촉구 방법을 제안

- **1B 모델의 문제점**
    - 1B 모델이 효과적인 CoT를 생성하는 것은 답변을 직접 생성하는 것보다 더 어려울 수 있음

## 3. Challenge of Multimodal-CoT

- 기존 연구: 1000억 개의 파라미터 이상에서 언어 모델에서 CoT 추론 능력이 나타날 수 있다고 제안
- 1B 모델에서 이러한 추론 능력을 끌어내는 것은 여전히 해결되지 않은 도전 과제
    - 1B 모델이 CoT 추론에서 실패하는 이유를 조사
    - 효과적인 접근 방식을 설계하는 방법을 연구

### 3.1 Towards the Role of CoT

- ScienceQA 벤치마크에 대한 CoT 추론을 위해 텍스트 전용 기준 모델을 미세 조정
- FLAN-AlpacaBase를 기본 언어 모델로 채택
    - 텍스트 생성 문제로 모델이 텍스트 정보를 입력으로 받아 합리적 설명과 답변으로 구성된 출력 시퀀스를 생성하도록 하는 것
- CoT의 효과를 연구하기 위해 세 가지 변형과 성능을 비교
    - 질문 텍스트(Q), 맥락 텍스트(C) 및 여러 선택지(M)의 토큰을 연결한 것을 입력
    - (i) No-CoT는 답변을 직접 예측하는 방식(QCM→A)
    - (ii) Reasoning은 답변 추론이 합리적 설명에 조건화된 방식(QCM→RA)
    - (iii) Explanation은 합리적 설명을 답변 추론을 설명하는 데 사용하는 방식(QCM→AR)

![Image](https://github.com/user-attachments/assets/5101de1a-c434-430b-b619-91327de3055e)

- 결과
    - **모델이 정답을 예측하기 전에 근거(rationales)를 예측하도록 설정하면(QCM → RA), 정확도가 12.31% 감소(81.63% → 69.32%)하는 것을 관찰**
        - 근거가 반드시 올바른 답을 예측하는 데 기여하지 않을 수도 있음을 암시
        - Lu et al. (2022a): 모델이 필요한 답을 얻기 전에 최대 토큰 한도를 초과하거나 예측 생성을 조기에 중단하기 때문
        - 그러나, 본 연구 생성된 출력(RA)의 최대 길이가 항상 400 토큰 미만, 언어 모델(T5 모델의 경우 512)의 길이 한도보다 낮다는 것을 발견

![Image](https://github.com/user-attachments/assets/cc2e7bab-dcd7-442d-8e4d-35f65e5e6816)

### 3.2 Misleading by Hallucinated Rationales

- 근거(rationales)가 답변 예측에 어떻게 영향을 미치는지 더 깊이 탐구
    - CoT 문제를 근거 생성(rationale generation)과 답변 추론(answer inference)의 두 단계로 나눔
    - RougeL 점수와 정확도
    - Table 3: 근거 생성에서 90.73 RougeL 점수를 달성했음에도, 답변 추론 정확도는 78.57%
    - Table 2: QCM→A 변형(81.63%)과 비교했을 때, 이러한 결과는 두 단계 프레임워크에서 생성된 근거가 답변 정확도를 향상시키지 못했음

![Image](https://github.com/user-attachments/assets/1d00c4fe-0642-424c-8b29-8d2794f9155f)

- 50개의 오류 사례를 무작위로 샘플링한 결과
    - 모델이 환각된(hallucinated) 근거를 생성하여 답변 추론을 잘못 이끄는 경향
    - Figure 2: 환각된 근거를 생성,  이는 시각적 콘텐츠에 대한 참조가 부족했기 때문

![Image](https://github.com/user-attachments/assets/2509d4f2-687b-4a01-90ae-3bc8c76d3cd3)

### 3.3 Multimodality Contributes to Effective Rationales

- 환각 현상이 효과적인 Multimodal-CoT를 수행하기 위해 필요한 시각적 맥락이 부족하기 때문이라고 추측
- 시각 정보를 주입하기 위한 간단한 방법: 이미지를 캡션으로 변환 후 두 단계의 입력에 추가하는 것
    - 그러나, Table3: 캡션만 사용하는 것은 미미한 성능 향상(↑0.80%)
    - → 언어 모델에 비전 기능을 통합하는 고급 기술을 탐색
        - 이미지를 ViT 모델에 입력, 시각 특징 추출
        - 디코더에 입력하기 전에 비전 기능과 인코딩된 언어 표현을 융합
- 시각 특징 사용한 결과
    - 근거 생성의 RougeL 점수가 93.46%로 증가 (QCM→R)
    - 이에 따라 답변 정확도가 85.31%로 향상 (QCMR→A)
    - = 환각 현상 완화 (60.7% 환각 에러 해결)
- 두 단계 방법이 일 단계 방법보다 더 나은 성능, Multimodal-CoT 프레임워크에서 두 단계 방법을 선택

## 4. Multimodal-CoT

- Multimodal-CoT를 제안
- motivation: 답변 추론이 다중 모드 정보를 기반으로 한 더 잘 생성된 근거를 활용할 수 있다는 예측

### 4.1 Framework Overview

- two operation stage 구성
    - (i) rationale generation, (ii) answer inference
    - 두 단계는 동일한 모델 구조를 공유하지만 입력 X와 출력 Y는 다릅

![Image](https://github.com/user-attachments/assets/4f468c2a-b36f-40a1-812b-2c71ab23cbe2)

- **rationale generation stage**
    - (수식 대략 편하게 씀 주의..)
    - 모델에 X = {X1언어, X비전}을 입력
        - X1언어: 첫 번째 단계의 언어 입력 / X비전은 비전 입력(이미지)
        - X는 질문, 맥락 및 여러 선택 문항의 옵션을 연결한 형태로 구성 가능
    - 목표: 근거 생성 모델 $R = F (X)$을 학습하는 것, R는 근거

- **answer inference stage**
    - 기존 X1언어에 근거 R을 추가 → 두 번째 언어 입력 X2언어 구성
        - 업데이트된 입력 $X' = \{X_2^{\text{language}}, X_{\text{vision}}\}$
        - 답변 추론 모델에 공급하여 최종 답변 $A = F (X')$ 추론

- 두 단계 모두, 동일한 아키텍처를 가진 두 모델을 독립적으로 학습
- 주석이 달린 요소들(예:  X → R, XR → A)을 훈련 세트에서 받아 감독 학습을 수행
- 추론 중에는 X가 주어졌을 때, 테스트 세트를 위한 근거: 첫 번째 단계에서 학습된 모델을 사용하여 생성, 두 번째 단계에서 답변 추론에 사용

### 4.2 Model Architecture

- 길이 N의 목표 텍스트 Y (근거 또는 그림 4의 답변) 생성을 위한 확률을 다음과 같이 계산

![Image](https://github.com/user-attachments/assets/aabb3248-036e-402a-bbff-51ab3acf161c)

- $p_{\theta}(Y_i \mid X_{\text{language}}, X_{\text{vision}}, Y_{<i})$
    - Transformer 기반 네트워크(Vaswani et al., 2017)를 사용하여 구현
    - 네트워크에는 세 가지 주요 절차: 인코딩, 상호작용 및 디코딩
    - 언어 텍스트를 Transformer 인코더에 공급하여 텍스트 표현 얻고
    - 비전 표현과 상호 작용 및 융합한 후 Transformer 디코더에 전달

- **인코딩**
    - 모델 F(X): 언어 및 비전 입력 받아 텍스트 표현 H_language 및 이미지 특징 H_vision을 얻음
    - LanguageEncoder: Transformer 모델로 구현, 트랜스포머 인코더의 마지막 층에서 은닉상태 언어 표현 사용
    - VisionExtractor:
        - 입력 이미지를 비전 특징 벡터화하는데 사용, ViT와 같이 고정된 비전 추출 모델에서 패치 수준의 특징 사용
        - 학습 가능한 프로젝트 행렬 W_h 를 적용
    - 이미지 없는 경우, “빈 특징” 으로 제로 벡터 사용

![Image](https://github.com/user-attachments/assets/73614b14-02e3-4fce-9bad-d292278c88cc)

- **상호작용 Interaction**
    - 단일-헤드 주의 메커니즘(Single-head Attention)
        - query (Q), key (K) and value (V): H_language, H_vision, H_vision
        - 텍스트와 이미지 사이의 상관관계를 찾기 위해 주의(attention) 메커니즘이 사용
        - 텍스트 토큰(단어 단위)과 이미지 패치(작게 나뉜 이미지 조각) 간의 연결을 계산, 텍스트의 특정 부분이 이미지의 어떤 부분과 관련이 있는지 파악
    - 게이트 융합 메커니즘(Gated Fusion Mechanism)
        - 텍스트와 이미지 특징을 결합
        - 텍스트와 이미지가 모두 중요할 경우, 두 특징이 균형 있게 융합되며, 이미지가 더 중요하다면 이미지 특징에 더 많은 가중치

![Image](https://github.com/user-attachments/assets/cba0bd45-0136-4eb1-8f99-bd16d7526bf0)

- **디코딩**
    - 융합된 출력 H_fuse: 변환기 디코더에 입력되어 목표 Y 예측

## 5. Experiments

### 5.1 Dataset

- ScienceQA, A-OKVQA 벤치마크 데이터셋 사용하여 평가
    - 추론 체인을 포함한 최신 다중 모달 추론 벤치마크

### 5.2 Implementation

- **실험 설정**
    - Base(200M) 및 Large(700M) 설정에서 T5 인코더-디코더 아키텍처 (Raffel et al., 2020)를 채택
    - FLAN-Alpaca를 사용하여 모델 가중치를 초기화
    - Multimodal-CoT가 UnifiedQA (Khashabi et al., 2020) 및 FLAN-T5 (Chung et al., 2022)와 같은 다른 백본 언어 모델과도 일반적으로 효과적임을 보여줄 것
    - 시각적 특징: 고정된 ViT-large 인코더 (Dosovitskiy et al., 2021b)를 통해 획득

- 베이스라인 모델
    - 세 가지 범주
    1. 비주얼 질문 답변 (VQA) 모델: 
        - 질문, 컨텍스트 및 선택지를 텍스트 입력, 이미지는 시각적 입력
        - 선택 후보에 대한 점수 분포를 예측하기 위해 선형 분류기를 사용
    2. 언어 모델 (LM): 
        - 텍스트-텍스트 UnifiedQA 모델: 이미지 캡셔닝 모델에서 추출된 캡션으로 변환, 텍스트 생성 문제로 취급
        - Few-shot 학습 LLM:  추론은 몇 가지 샷 프롬프트를 기반, 테스트 인스턴스 이전에 훈련 집합에서 두 개의 인컨텍스트 샘플이 연결
    3. Fine-tuned large vision-language model

### 5.3 Main Results

- Table4: ScienceQA 벤치마크에서의 주요 결과
    - Mutimodal-CoTLarge가 이전 발표된 최고의 모델보다 상당한 성능 향상을 달성했음을 발견(86.54%→90.45%)
- Table5: A-OKVQA 벤치마크에서 얻은 결과, 더욱 효과적

![Image](https://github.com/user-attachments/assets/204b7508-6ebf-4b29-8fc0-0c92d63a69cd)
![Image](https://github.com/user-attachments/assets/ddd671fc-6995-4a0a-8fe2-8ec10f850e8d)


- 주목할만한 점
    - Chameleon, LLaMA-Adapter, LLaVA, InstructBLIP과 같은 연구: 우리의 연구 이후 몇 개월 뒤에 발표된 동시대 작업이라는 것
        - 우리의 방법: 이러한 멀티모달 모델들(예: InstructBLIP)과 직교적(orthogonal) 관계
        - 직교적 관계: 각 모델이 서로 방해되지 않고 독립적으로 기능하며, 동시에 조합하여 사용 가능

- Table 6: ablation study
    - 시각적 특징(vision features)의 통합과 2단계 프레임워크(two-stage framework) 설계 효과 모두 입증
    - Multimodal-CoT는 환각 문제를 완화
    - 수렴 속도 개선

![Image](https://github.com/user-attachments/assets/6ffd4c61-4520-4fa3-a735-bb8a0674fa8b)

## 6. Analysis

- Multimodal-CoT가 수렴 속도를 향상
- 인간 주석이 없는 합리성의 시나리오에 적응할 수 있는 가능성 입증
- 다양한 백본 모델 및 비전 기능과 함께 Multimodal-CoT의 일반적인 효율성을 조사
- 한계점을 탐색하고 향후 연구에 영감을 주기 위해 오류 분석을 수행

### 6.1 Multimodality Boosts Convergence

- Figure5: No-CoT 베이스라인과 Multimodal-CoT 변형의 검증 정확도 곡선
    - One-stage: 최상의 성능을 보인 QCM→A 입력-출력 형식을 기반
    - Two-stage: 본 연구의 두 단계 프레임워크
    - 두 단계 방법이 CoT 없이 답변을 직접 생성하는 일단계 베이스라인보다 초반에 상대적으로 높은 정확도를 달성
    - 그러나, 비전 기능이 없으면 두 단계 베이스라인은 낮은 품질의 합리성으로 인해 훈련이 진행됨에 따라 더 나은 결과 X

### 6.2 When Multimodal-CoT Meets Large Models

- Multimodal-CoT 모델이 인간 주석 데이터에 의존하지 않고, 대형 모델을 통해 생성된 rationales 을 사용할 수 있는지 확인하려는 것
- 기존 방식 (Multimodal-CoT w/ Annotation): 벤치마크 데이터셋(ScienceQA)의 인간 주석된 rationales을 사용하여 모델을 훈련
- 대체 방식 (Multimodal-CoT w/ Generation):
    - 인간 주석 대신 **InstructBLIP**와 **ChatGPT**를 활용하여 rationales을 생성
    - 생성된 두 가지 rationales을 결합해 모델을 훈련
- Table7: 생성된 rationales 사용하면 훈련에 대한 인간 주석 rationales 사용과 유사한 성능, 베이스라인 모델에 직접 질문하는 것보다 우수

![Image](https://github.com/user-attachments/assets/f19cb6bb-c341-4c03-9b54-3a30fc108d60)

### 6.3 Effectiveness Across Backbones

- 다른 백본 모델에 대한 일반성 테스트: 기본 LMs를 다른 유형의 변형으로 변경
    - 전반적으로 다 Prior Best 보다 성능 향상

![Image](https://github.com/user-attachments/assets/70edee9c-83af-4c1b-b780-85b623de2ecb)

### 6.4 Using Different Vision Features

- 다양한 비전 특징은 모델 성능에 영향
- 세 가지 널리 사용되는 유형
    - ViT, CLIP 및 DETR: 패치와 같은 특징
    - ResNet 특징의 경우: ResNet-50의 풀링된 특징을 텍스트 시퀀스와 같은 길이로 반복하여 패치와 같은 특징을 흉내
- ViT가 상대적으로 더 나은 성능을 달성

![Image](https://github.com/user-attachments/assets/661dce5f-64c0-4ad7-9028-6ecccaea55f4)

### 6.5 Alignment Strategies for Multimodal Interaction

- 멀티모달(CoT) 모델에서 서로 다른 정렬 방식(alignment strategies)이 멀티모달-Chain of Thought(Multimodal-CoT) 모델의 성능과 행동에 어떻게 영향
    - 정렬 전략: 텍스트와 이미지 정보가 어떻게 결합되고 상호작용하는지를 조정하는 방식
    - 새로운 정렬 전략: BLIP 모델(Li et al., 2022b)
    - 이미지 기반 텍스트 인코더(image-grounded text encoder) 사용
    - Transformer 블록의 Self-Attention Layer와 Feed-Forward Network사이에 추가적인 Cross-Attention 층을 삽입
- Table 10: BLIP의 이미지 기반 텍스트 인코더를 사용하는 정렬 전략이 텍스트와 이미지를 단순히 결합하여 직접 답변을 생성하는 방식(Direct Answering)보다 성능 향상
    - Image-grounded text encoder (Blip)
    - Unimodal Encoder (본 논문)

![Image](https://github.com/user-attachments/assets/7e37c03b-0238-4c55-9b93-3e01ebd32efa)

### 6.6 Generalization to Other Multimodal Reasoning Benchmarks

- 다중 모드 추론 벤치마크인 MMMU(Yue et al., 2024)를 활용
- 추가 훈련 없이 MMMU에서 Multimodal-CoT에 대한 평가를 수행
- Multimodal-CoT는 MMMU에 효과적으로 일반화하여 약 8B 크기의 다양한 더 큰 모델보다 더 나은 성능

![Image](https://github.com/user-attachments/assets/1e4c83f2-058c-4d69-858f-f677afb2e2f6)

### 6.7 Error Analysis

- 무작위 선택된 예제를 수동으로 분석
    - 잘못된 답변을 산출한 50개의 샘플을 조사하고 그에 따라 분류

- 상식적 실수
    - 오류의 가장 일반적인 유형
    - 오류의 80%를 차지
    - 모델이 지도 해석, 이미지 내 객체 수 세기, 알파벳 활용 등 상식적 지식이 필요한 질문에 직면했을 때 발생
- 논리적 실수
    - 오류의 14%를 차지
    - 추론 과정에서 모순
- CoT가 비어 있거나 옳은 경우에도 잘못된 답변이 제공되는 경우도 관찰
    - 오류의 6%
    - CoT는 최종 답변에 반드시 영향을 미치지 않을 수 있음

- **향후 연구**
    - 더 많은 정보성 있는 시각적 특징을 통합, 언어와 비전 간의 상호작용을 강화하여 enable comprehension of maps and numerical counting
    - 상식적 지식을 통합
    - 관련 CoT만 사용하여 답변을 추론하고 무관한 것은 무시하는 필터링 메커니즘을 구현

## 7. Conclusion

- Multimodal-CoT를 제안: 언어와 비전 모드를 두 단계 프레임워크에 통합
    - 이유 생성과 정답 추론을 분리
    - 정답 추론이 다중 모드 정보에서 더 잘 생성된 이유를 활용
- 성능
    - ScienceQA 벤치마크에서 최첨단 성능을 달성
    - 환각 현상을 완화하고 수렴 속도를 향상
- 향후 연구
    - CoT 추론을 개선하기 위해 더 효과적인 비전 특징을 활용하고, 상식 지식을 주입하며, 필터링 메커니즘을 적용할 가능성을 식별