# How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites

날짜: 2025년 1월 23일

- [https://arxiv.org/pdf/2404.16821](https://arxiv.org/pdf/2404.16821)
- CVPR 2024 Oral
- LLaVA 아키텍처가 아닌 InternVL 을 사용한 논문이 있어서 궁금해서 읽어봄
    - LLaVA 랑 다른 점은 인코더를 강화시키는 부분 (파라미터 크게, 지속적으로 학습 강조, freeze 안하고 같이 학습함)
    - 이 논문은 InternVL 1.5인데 현재는 2.5까지 나온 상황
    - LLaVA github star 21.2k, InternVL 6.9k

## Abstract

- **InternVL 1.5**
    - 오픈 소스 멀티모달 대규모 언어 모델(MLLM)
    - 오픈 소스 모델과 상용 모델 간의 능력 격차를 해소하기 위한 것
    - 일련의 벤치마크 및 비교 연구를 통해 InternVL 1.5의 성능을 평가
    - 18개의 멀티모달 벤치마크 중 8개에서 최첨단 결과를 달성

- **세 가지 간단한 개선 사항**
    - 강력한 비전 인코더
        - 대규모 비전 기반 모델인 InternViT-6B에 대한 지속적 학습 전략 continuous learning strategy을 탐색
        - 시각적 이해 능력을 향상, 다양한 LLM에서 이전하고 재사용 가능
    - 동적 고해상도
        - 입력 이미지의 종횡비와 해상도에 따라, 448×448 픽셀의 타일로 이미지를 1에서 40까지 나누어, 최대 4K 해상도의 입력을 지원
    - 고품질 이중 언어 데이터셋
        - 일반적인 장면과 문서 이미지에 대한 고품질 이중 언어 데이터셋을 신중하게 수집
        - 영어 및 중국어 질문-답변 쌍으로 주석 처리하여 OCR 및 중국어 관련 작업에서 성능을 크게 향상



## 1. Introduction

- **멀티모달 대규모 언어 모델(MLLM)**
    - 텍스트와 시각 정보 간의 격차를 해소하는 데 기여
    - 오픈 소스 모델과 상용 모델 간의 능력에는 여전히 눈에 띄는 차이
        - (1) 매개변수 규모:
            - 최근의 독점 상업 MLLM은 일반적으로 1000억 개 이상의 매개변수
            - 오픈 소스 모델은 통상 3억 개 매개변수를 가진 비전 기초 모델(VFM)을 사용
            - 70억 또는 130억 개 LLM과 통합
        - (2) 이미지 해상도:
            - 독점 상업 모델: 일반적으로 원래의 종횡비 유지, 세밀한 장면 및 문서 이해를 용이하게 하는 동적 해상도 접근 방식 사용
            - 오픈 소스 모델: 고정 해상도(예: 336×336 및 448×448)로 훈련, 상업적 모델에 비해 상당한 능력 차이
        - (3) 다국어 능력:
            - 독점 모델: 종종 다양한 언어에 대한 성능을 향상시키기 위해 광범위한 다국어 데이터셋을 활용
            - 오픈 소스 모델: 주로 영어 데이터를 사용하고, 다른 언어에 대해서는 LLM의 제로샷 능력에 의존. 비영어 이해 및 OCR 작업에서 최적 이하의 성능

- **InternVL 1.5**
    - 오픈소스, 독점 모델 간의 격차 해소
    - 세 가지 주요 개선점
        - (1) 대규모 VFM인 InternViT-6B
            - 지속적인 학습 방식을 구현, 고품질 이미지-텍스트 데이터로 개선
            - 모델의 시각적 콘텐츠 이해 능력을 향상시킬 뿐만 아니라 다양한 LLM에 대한 적응성을 개선
            - InternLM2-20B 언어 기초 모델로 사용, 언어 처리 능력 제공
        - (2) 이미지의 종횡비와 해상도에 따라 448×448 타일로 분할하는 동적 고해상도 전략을 채택
            - 타일 수는 1개에서 40개(즉, 4K 해상도)까지 다양
            - 글로벌 컨텍스트를 포착하기 위해 썸네일 뷰도 추가
        - (3) 고품질 자연 장면, 차트, 문서 및 영어 및 중국어 대화를 포괄하는 다양한 공개 데이터셋을 수집
            - 오픈 소스 LLM을 사용한 데이터 번역 파이프라인을 개발하여 더 많은 언어로 쉽게 확장
    - 모델에 몇 가지 장점
        - (1) 유연한 해상도
            - GPT-4V의 “저해상도” 또는 “고해상도” 모드와 유사하게, InternVL 1.5는 사용자에게 이미지에 최적의 해상도 선택 가능
            - 장면 주제 설명을 위한 저해상도와 문서 이해를 위한 고해상도(최대 4K 해상도)를 사용
            - 계산 효율성과 세부 정보 보존 간의 균형을 효과적으로 유지
        - (2) 이중 언어 능숙도
            - InternVL 1.5는 강력한 이중 언어 능력을 발휘, 영어와 중국어
        - (3) 강력한 시각적 표현
            - InternViT-6B 의 시각적 표현 능력 강화, 다양한 시각적 도메인에서도 유연한 입력 해상도
            - InternViT-6B의 방대한 매개변수, InternVL 1.5는 200억 개 이상의 매개변수를 가진 LLM의 언어적 능력과 겨룰 수 있는 수준의 시각적 표현을 달성
    - 18개의 대표적인 다중 모드 벤치마크에서 InternVL 1.5를 평가
        - OCR 관련, 일반 다중 모드, 수학 및 멀티턴 대화 벤치마크로 분류
        - 18개의 벤치마크 중 8개에서 최첨단 성과를 달성
        - OCR 관련 데이터셋에서 독점 모델 초월하는 성과



## 2. Related Work

### 2.1 Proprietary Commercial MLLMs

- **독점 상업 MLLMs**
    - OpenAI의 GPT-4V:  시각적 입력을 포함, GPT-4의 능력을 확장하여 텍스트와 이미지 콘텐츠를 모두 처리 가능
    - 구글의 Gemini 시리즈: Gemini 1.0에서 Gemini 1.5로 발전, 텍스트, 이미지, 오디오를 처리,  최대 100만 개의 토큰을 지원
    - Qwen-VL 시리즈: Qwen-VL-Plus/Max는 알리바바의 주요 모델, OCR 도구 없이도 멀티모달 작업 우수
    - 등


### 2.2. Open-Source MLLMs

- **오픈 소스 MLLMs**
    - 일반적으로 336×336 또는 448×448과 같은 작은 고정 해상도의 이미지를 기반으로 훈련
    - 비정상적인 종횡비 또는 문서 데이터를 가진 이미지에서 서브 최적의 성능
- **고해상도 이미지에서 훈련 방식 제안됨**
    - Dual-branch image encoder
    - 다른 하나는 고해상도 이미지를 많은 저해상도 타일로 나누는 방법
- 그럼에도, 독점 상업 모델과 차트 및 인포그래픽 이해뿐 아니라 장면 텍스트 인식에서 여전히 상당한 격차


### 2.3. Vision Foundation Models for MLLMs

- **비전 기초 모델(VFMs)**
    - MLLMs에 가장 적합한 비전 인코더를 찾기 위한 많은 연구가 진행
    - 현재 CLIP-ViT및 SigLIP와 같은 모델이 널리 사용
    - Tong et al. [111]: CLIP과 DINOv2의 시각적 패턴에서 눈에 띄는 차이, 두 VFM을 결합하는 혼합 피쳐 모듈을 개발
    - LLaVA-HR: 저해상도 경로에 CLIP-ViT를 사용, 고해상도 경로에 CLIP-ConvNext를 활용하는 Dual-branch image encoder 도입
    - DeepSeek-VL:  저해상도 이미지를 위해 SigLIP-L을 사용, 고해상도 이미지를 위해 SAM-B를 사용 dual vision encoder design
    
- **비전 기초 모델인 InternViT-6B**
    - 지속적 학습 전략을 제안



## 3. InternVL 1.5

### 3.1 Overall Architecture

- **ViT-MLP-LLM: 구성과 유사한 아키텍처를 채택**
    - 미리 훈련된 InternViT-6B와 미리 훈련된 InternLM2-20B를 임의 초기화된 MLP 프로젝터를 사용하여 통합
- **이미지 크기**
    - 1에서 12까지 다양한 448×448 픽셀 타일로 나누는 동적 해상도 전략을 구현
    - 제로샷으로 40개의 타일(즉, 4K 해상도)로 확장
    - 고해상도 확장을 위해 시각적 토큰 수를 원래의 1/4로 줄이기 위해 단순히 픽셀 셔플 연산을 사용
    - 448×448 이미지는 256개의 시각적 토큰으로 표현

### 3.2. Strong Vision Encoder

- **기존 MLLM의 기존 비전 기초 모델 ViT**
    - 일반적으로 인터넷에서 크롤링된 이미지-텍스트 쌍에 대해 고정된 낮은 해상도(예: 224×224)로 훈련
    - 고해상도 이미지를 처리하거나 문서 이미지와 같은 인터넷 외부의 소스로부터 이미지를 처리시 성능 저하

- **InternViT-6B-448px-V1.2**
    - InternVL 1.2 업데이트: InternViT-6B 모델의 지속적인 사전 훈련이 포함
        - 네 번째 마지막 레이어의 피쳐가 멀티모달 작업에 가장 잘 작동하는 것 발견
        - 마지막 세 레이어의 가중치는 직접 폐기하고 InternViT-6B를 48 레이어에서 45 레이어로 감소
        - 이후, InternViT-6B의 해상도를 224에서 448로 증가, Nous-Hermes-2-Yi-34B와 통합
        - 모델에 고해상도 처리 및 OCR 기능을 갖추기 위해, 비전 인코더와 MLP 모두 활성화
        
- **InternViT-6B-448px-V1.5**
    - InternViT-6B-448px-V1.2의 강력한 기초의 사전 훈련을 계속
    - 이번 업데이트에서 훈련 이미지의 해상도는 고정된 448×448에서 동적 448×448으로 확장
    - 기본 타일 크기는 448×448이고 타일 수는 1에서 12 사이로 변동
    - 사전 훈련 데이터셋의 규모, 품질 및 다양성을 향상

- InternVL 1.5에서 LLM이 Nous-Hermes-2-Yi-34B에서 InternLM2-20B로 변경
    - 새로운 LLM과의 우수한 호환성과 이동성을 유지
    - InternViT-6B가 MLLM의 사전 훈련 단계에서 학습한 시각적 기능이 폭넓게 적용 가능, 특정 LLM에 밀접하게 결합되어 있지 않음


### 3.3 Dynamic High-Resolution

- **동적 고해상도 훈련 접근 방식**
    - UReader에서 영감
    - 입력 이미지의 다양한 해상도와 종횡비에 효과적으로 적응하는 동적 고해상도 훈련 접근 방식을 채택
    - 이미지를 타일로 분할하는 유연성을 활용, 모델이 다양한 이미지 해상도를 수용, 상세한 시각 정보를 처리할 수 있는 능력을 향상

- **Dynamic Aspect Ratio Matching**
    - 자연스러운 종횡비를 유지하기 위해, 미리 정의된 종횡비 세트에서 최적의 종횡비를 동적으로 맞춤
    - 제한된 계산 자원으로 인해 훈련 중 최대 12개의 타일 허용
        - 1에서 12개의 타일로 형성된 35가지 가능한 종횡비 조합 포함
        - 매칭 과정: 각 입력 이미지의 종횡비 계산, 35개의 미리 정의된 종횡비와 비교하여 절대 차이를 측정
        - 여러 개의 미리 정의된 종횡비가 일치하는 경우(예: 1:1 및 2:2), 입력 이미지 면적의 두 배를 초과하지 않는 종횡비를 우선적으로 선택

- **Image Division & Thumbnail**
    - 적절한 종횡비 결정, 이미지는 해당 해상도로 크기 조정
        - 800×1300 이미지는 896×1344로 조정
    - 조정된 이미지는 448×448 픽셀의 타일로 나뉨, 타일과 함께 전체 이미지의 썸네일도 포함되어 전체 맥락을 포착
        - 이 썸네일은 448×448로 축소되어, 모델이 전반적인 장면을 이해하는 데 도움
    - 훈련 중 시각 토큰의 수는 256에서 3,328까지 변동
    - 테스트 중 최대 40개의 타일이 가능하여 10,496개의 시각 토큰을 생성 가능



### 3.4 High-Quality Bilingual Dataset

![Image](https://github.com/user-attachments/assets/c654b8c6-12aa-4068-9290-0a01021cdc8e)

- **Pre-training Dataset**
    - 다양한 공개 데이터 소스, 다양한 태스크를 포함, 캡셔닝, 객체 검출 및 그라운딩, OCR 등
    - 캡셔닝 (Captioning): 전체 데이터의 53.9%
    - 객체 검출 및 그라운딩 (Detection and Grounding): 전체 데이터의 5.2%
    - OCR: 전체 데이터의 32.0%,  PaddleOCR을 사용해 Wukong(중국어 이미지)과 LaionCOCO(영어 이미지)에서 OCR 작업을 수행해 구축.
    - 소규모 OCR 데이터셋: 더 구체적이거나 제한된 OCR, 전체의 8.9%

- **Fine-tuning Dataset**
    - 이미지 캡션: 이중 언어 서술적 캡션 생성
    - 일반 Q&A 분야: 모델이 다양한 질문-응답 시나리오 처리
    - 과학적 이미지 이해: 과학 도표 및 텍스트 해석 능력 향상, 차트 해석, 수학 데이터셋, 지식 기반 Q&A 등
    - OCR: 이미지에서 텍스트 인식을 개선, 문서 이해, 비주얼 그라운딩
    - multimodal conversation: 모델의 대화 능력
    - text-only dataset: LLM의 원래 언어 능력을 유지하는 데 사용

<aside>
💡

**Visual Grounding**

이미지와 텍스트 정보를 연결하는 기술로, 주어진 텍스트(예: 문장이나 질문)와 관련된 특정 객체나 영역을 이미지 안에서 정확히 식별하고 매칭하는 작업

텍스트를 기반으로 이미지를 이해하고, 이미지에서 관련 부분을 찾아내는 것

</aside>

- **Data Translation Pipeline**
    - 모델의 다국어 능력을 향상시키기 위해 데이터 번역 파이프라인 구현
    - 최신 오픈 소스 LLM 또는 GPT-3.5를 활용하여 영어 데이터셋을 다른 언어(예: 중국어)로 변환
    - 수동 주석 프로세스에 의존하지 않고, 언어 프롬프트를 조정하여 더 많은 언어를 포함하도록 손쉽게 확장 가능
    - 각 데이터셋에 대한 언어를 주석 처리 (표1 데이터셋별 괄호)
        - 원래 영어로 되어 있는 데이터셋에 대해 "zh"라는 주석, 번역 파이프라인을 사용하여 그것을 중국어로 번역
        - 번역 파이프라인을 활용, InternVL 1.5의 중국어 처리 능력이 크게 향상

![Image](https://github.com/user-attachments/assets/2fd608c0-911d-491b-8170-7ce462526020)



## 4. Experiments

### 4.1 Implementation Details

- InternViT-6B 비전 인코더와 InternLM2-20B 언어 모델을 동적 고해상도 전략으로 통합하여 개발
    - 이미지는 448×448 픽셀 타일로 분할
    - 타일의 수는 학습 중 이미지의 종횡비와 해상도에 따라 최대 12개
    - 테스트 단계에서 모델은 40개의 타일, 즉 4K 해상도에 해당하는 입력을 처리 가능
    - LLM이 기본 모델이 아닌 InternLM2-20B의 대화형 버전을 기반으로 모델 구축

- **학습 두 단계**
    1. 비전 인코더인 InternViT-6B 및 MLP 프로젝터의 학습에 중점, 시각적 특징 추출을 최적화
    2. 모델 전체의 260억 개 파라미터가 미세 조정, 다중 모달 기능 향상
    - 두 단계의 교육 모두에서 4096의 컨텍스트 길이 사용
    - LLaVA 1.5와 동일한 응답 형식 프롬프트를 채택
    - 평가는 주로 VLMEvalKit 사용
        - 대규모 비전-언어 모델(LVLM)을 평가하기 위한 오픈 소스 툴킷
        - 70개 이상의 다양한 대규모 멀티모달리티 모델과 20개 이상의 멀티모달 벤치마크를 지원
        - 새로운 모델을 쉽게 추가 가능, 데이터 준비, 분산 추론, 예측 후처리, 메트릭 계산 등의 작업을 자동으로 처리


### 4.2. Comparison with State-of-the-Art MLLMs

![Image](https://github.com/user-attachments/assets/1b2dabc6-edee-49ee-9186-0053370aa33f)

- 다중 모달 성능을 평가하기 위해 여러 벤치마크에 대한 광범위한 평가를 수행
    - 네 가지 유형: OCR 관련, 일반 다중 모드, 수학, 멀티턴 대화
    - InternVL 1.5는 이러한 벤치마크의 대부분에서 우수한 성능

- **OCR-related Image Understanding**
    - 네 가지 핵심 부분: 문서 이해, 차트 이해, 인포그래픽 이해, 장면 텍스트 해석
    - OCRBench: 모델의 전반적인 OCR 능력을 포괄적으로 평가
    - 독점 모델과 유사한 성능, LLaVA-NeXT 및 InternVL 1.2의 오픈 소스 모델을 크게 초과
    - ChartQA 및 OCRBench에서 최첨단 성능을 달성, 모든 경쟁 독점 모델을 능가

- **General Multimodal Evaluation**
    - 구체적 데이터셋 사항
        - RealWorldQA: 모델의 실제 세계 공간 이해 능력을 평가
        - HallusionBench: 환각 제어 능력을 평가하는 데 사용
        - MMMU: 모델의 다학제 능력을 평가하는 데 사용
        - AI2D: 과학 다이어그램 이해를 평가하는 데 활용
        - MMBench-CN, CCBench: 모델의 중국어 능력 및 중국 문화 이해를 평가
        - 기타 포괄적 벤치마크도 모델의 시각적 이해 및 추론 능력을 평가하기 위해 사용
    - InternVL 1.5는 이러한 벤치마크에서 독점 모델과의 간격을 크게 줄임
    - MMMU, MMT-Bench에서 InternVL 1.2에 비해 약간 감소, 작은 감소는 언어 모델 크기가 작아진 데 기인

- **Math Reasoning**
    - MathVista: 다양한 수학 및 시각적 작업에서의 과제를 통합하기 위해 설계된 벤치마크
    - 독점 상업 모델이 상당한 어려움을 겪는 영역
    - 이 벤치마크에서 GPT-4V를 포함한 다른 모델들을 명확하게 초과, 수학적으로 까다로운 작업을 처리하는 능력 입증

- **Multi-Turn Conversation**
    - 단일 턴 대화에 비해 다중 턴 대화는 인간의 선호와 더 일치
    - ConvBench: MLLMs의 인식, 추론 및 창의력 능력을 점진적으로 평가하는 다중 턴 대화를 평가
    - InternVL은 오픈소스 모델 중에서 우수한 성능을 나타내지만, 여전히 GPT-4V에 비해 상당한 차이
    - 다중 턴 대화에서 InternVL의 기능을 계속 개선 예정

![Image](https://github.com/user-attachments/assets/3a4e7ca0-d788-470f-b97e-a8274c8a0d9b)



### 4.3 Ablation Study

- **Larger LLMs need Larger VFMs**
    - LLM과 VFM 간의 상호작용을 조사
    - 두 개의 오픈소스 MLLM: LLaVA-NeXT, InternVL 1.2
        - 각각 340억 개의 파라미터를 가진 LLM을 장착
        - InternVL 1.2는 LLaVA-NeXT의 3억 개의 파라미터에 비해 60억 개의 파라미터를 가진 훨씬 더 큰 VFM을 포함
    - LLaVA-NeXT의 데이터 사용 불가, 유사한 데이터셋을 직접 생성
    - InternVL 1.2는 고정 해상도 448×448에서 훈련, LLaVA-NeXT는 더 높은 동적 해상도 672 × 672를 사용
    - → 완전 공정하거나 동등하지 X
    - 그럼에도 불구하고, 유의미한 관찰
        - 다섯 개의 OCR 관련 데이터셋, ConvBench 및 RealWorldQA를 제외한 뒤에, InternVL 1.2는 나머지 11개 데이터셋 중 9개에서 LLaVA-NeXT보다 뛰어난 성능을 발휘
        - = 대규모 LLM(예: 34B)의 경우 더 큰 VFM(예: 6B)이 모델이 복잡한 다중 모드 작업을 처리하는 능력을 효과적으로 개선 가능

- **Dynamic Resolution Matters**
    - 다양한 다중 모드 벤치마크에 걸쳐 동적 해상도의 효과를 조사
    - 모든 작업이 높은 해상도를 요구하는 것은 X
    - DocVQA, InfoVQA, TextVQA 및 OCR-Bench와 같은 OCR 관련 작업은 해상도 증가의 이점
    - AI2D, MMMU, MMBench 및 HallusionBench와 같은 작업은 높은 해상도에서 성능이 약간 감소하는 현상
    - **InternVL 1.5**는 동적 해상도에 대한 높은 강건성
        - 작업의 특정 요구 사항에 따라 해상도를 조정 가능
        - 높은 해상도가 유리한 경우 최적의 성능을 보장, 그렇지 않은 경우 리소스를 절약 가능

![Image](https://github.com/user-attachments/assets/20c83908-0508-4f13-a15c-68092294c334)



### **4.3.1 Qualitative Results on Different Scenes**

- 다양한 시나리오에서 GPT-4V와 우리의 모델을 질적으로 비교
- 일반 QA, OCR 관련 QA, 과학적 이해, 중국 전통 문화, 물체 위치 지정, 다중 이미지 대화 등을 포함

- **General QA**
    - 두 모델 모두 쿼리에 정확하게 응답하며 일반 주제에 대한 능력
    - GPT-4V는 개인 개인정보와 관련된 질문에 대해 지나치게 답변을 거부

![Image](https://github.com/user-attachments/assets/e98c5fce-2598-4006-91bb-7eb055304c5e)


- **OCR-Related QA**
    - 중국 장면을 이해하는 능력을 측정: GPT-4V는 이미지에서 유용한 정보를 모두 추출 불가
    - 차트 이해: 모두 좋은 성능

![Image](https://github.com/user-attachments/assets/18e6f6b8-413a-40ec-b86d-7d7ed291b33b)


- **Scientific Understanding**
    - 과학적 이해 추론 작업에서 모델의 능력을 평가하는 것은 계산 지능의 발전에 필수적
    - 첫 번째 질문: 모두 정확
    - 두 번째 질문: InternVL 1.5은 이미지에 나타난 요소를 정확하게 분석하고 올바른 응답, GPT-4V는 아미노산 수송의 추세에 대해 추측 (정답)
    - InternVL 1.5, GPT-4V 과학적 이해 및 추론에 있어 유사한 능력

![Image](https://github.com/user-attachments/assets/3401a8a0-972d-471e-a18f-32b3574aedae)


- **Chinese Traditional Culture**
    - InternVL 1.5와 GPT-4V 모두 이미지에 묘사된 중국 전통 문화를 올바르게 인식
    - InternVL 1.5는 이 문화를 더 깊이 이해 (more detailed descriptions)

![Image](https://github.com/user-attachments/assets/ca6ebe2d-5a04-4390-babb-7aa916136bc6)


- **Object Localization**
    - 객체 로컬라이제이션 작업에서의 능력을 평가하는 것은정확한 공간 인식이 요구되는 응용에서 매우 중요
    - 복잡한 여러 개체 간의 동적 상호작용을 포함한 복잡한 시나리오에서어지러운 장면 내의 간단한 객체 인식에 이르기까지 다양
    - 객체를 높은 정확도로 로컬라이즈할 뿐만 아니라공간 관계에 대한 이해도 비슷한 수준을 보여주어 GPT-4V의 성능과 일치

![Image](https://github.com/user-attachments/assets/f89ea9ee-7c5a-41fa-a4be-a26e5173aa3b)

- **Multi-Image Dialogue**
    - InternVL 1.5와 GPT-4V에게 두 이미지를 비교하도록 요청
    - InternVL 1.5는 단일 이미지 입력만으로 훈련되었음에도
    - 다중 이미지 대화에 대한 강력한 제로샷 능력을 보여준다는 것을 발견

![Image](https://github.com/user-attachments/assets/10777123-5c7d-41f9-ba02-2ad03232ec4b)



## 5. Conclusion

- 오픈소스 MLLM InternVL 1.5
    - 강력한 비전 인코더를 통합, 지속적인 학습 능력
    - 동적 고해상도 전략을 채택하고, 고품질 이중언어 데이터셋을 활용
- 선도적인 독점 모델들과 경쟁할 만한 성능
- OCR 관련 작업에서 우수한 결과를 기록
- 중국어 관련 장면 이해에서 상당한 개선