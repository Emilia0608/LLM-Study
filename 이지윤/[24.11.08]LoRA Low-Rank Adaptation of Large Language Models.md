# LoRA: Low-Rank Adaptation of Large Language Models

날짜: 2024년 11월 8일

[https://arxiv.org/abs/2106.09685](https://arxiv.org/pdf/2106.09685)

## Abstract

- LoRA는 사전 학습된 모델 가중치를 고정하고, 각각의 Transformer 아키텍처 레이어에 학습 가능한 순위 분해 행렬을 주입하여 다운스트림 작업을 위한 학습 가능한 매개변수 수를 크게 줄입니다.
- (Adam으로 미세 조정된 GPT-3 175B와 비교) LoRA는 학습 가능한 매개변수 수를 10,000배 감소시키고 GPU 메모리 요구 사항을 3배 줄입니다.
- LoRA는 학습 가능한 매개변수가 적음에도 불구하고 RoBERTa, DeBERTa, GPT-2, GPT-3에서 모델 품질 면에서 미세 조정보다 동등하거나 더 나은 성능

- 기반 아이디어: 중요한 매개변수가 더 크게 변화한다
    - 낮은 "내재 차원”:  모델이 학습하는 동안 중요한 정보를 반영하는 데 필요한 매개변수의 수가 전체 매개변수 수보다 상대적으로 적다는 것을 의미
    

## Introduction

- 자연어 처리 → 사전 학습된 언어모델 다운스트림 파인튜닝
    - 파인튜닝 단점: 새로운 모델이 원래 모델만큼 많은 매개변수를 포함
- 많은 연구자들이 몇몇 매개변수만 적응시키거나 새로운 작업을 위한 외부 모듈을 학습하는 방법으로 이를 완화
    - 각 작업에 대해 사전 학습된 모델 외에 소수의 작업별 매개변수만 저장하고 로드하면 되므로 배포 시 운영 효율성을 크게 향상 가능
        
        → 종종 모델의 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄임으로써 추론 지연(latency)을 초래
        
        = 파인튜닝 기준을 충족하지 못해 효율성과 모델 품질 사이의 균형을 맞추는 어려움
        
- 학습된 과잉 파라미터 모델이 실제로 낮은 내재 차원에 위치
    - 모델 적응 과정 중 가중치의 변화도 낮은 “내재적 계급(rank)”을 가지고 있다고 가정
    - LoRA는 사전 학습된 가중치를 동결한 상태에서 적응 과정 중 밀집 레이어의 변화를 최적화하기 위해 밀집 레이어의 계급 분해 행렬을 최적화함으로써 신경망의 일부 밀집 레이어를 간접적으로 훈련
        
        → LoRA는 저장 및 계산 효율성
        
        ![image](https://github.com/user-attachments/assets/7265f506-d44c-4e90-9e44-b234d7625f3a)
        

- LoRA의 여러 가지 주요 이점
    - 사전 학습된 모델을 공유하고 다양한 작업을 위한 여러 작은 LoRA 모듈을 구축하는 데 사용 가능
        - 저장 요구 사항과 작업 전환 오버헤드를 크게 줄이기 가능
    - 적응형 최적화기를 사용할 때 하드웨어 진입 장벽을 최대 3배 낮춤
        - 대부분의 매개변수에 대해 그래디언트를 계산하거나 최적화기 상태 유지 필요 X
        - 주입된 훨씬 더 작은 저계급 행렬만 최적화
    - 간단한 선형 설계로 인해 배포할 때 훈련 가능한 행렬을 동결된 가중치와 결합 가능
        - 완전히 미세 조정된 모델에 비해 추론 지연을 발생 X
    - 이전 방법들과 직교적(orthogonal), 접두사 조정(prefix-tuning)과 같은 다양한 방법들과 결합 가능

## 2. Problem Statement

- 사전 훈련된 자기회귀 언어 모델 PΦ(y∣x)
    - Φ 는 모델의 파라미터
    - 기계 독해(MRC), 자연어를 SQL로 변환(NL2SQL)과 같은 다운스트림 태스크에 적용
    - 컨텍스트-타겟 쌍의 훈련 데이터셋 $Z={(xi​,yi​)}$
        - $xi, yi$  는 토큰 시퀀스
        - $xi$는 자연어 쿼리, $yi$는 해당 SQL 명령
    - 전체 파인 튜닝, 전 훈련된 가중치 $Φ_0$로 초기화, 반복적으로 그래디언트를 따라 업데이트 $Φ_0​+∆Φ$로 업데이트
        - 단점: 다운스트림 작업마다 다른 파라미터 세트 $∆Φ$를 학습, 차원 $∣∆Φ∣=|Φ_0|$
    - 작업별 매개변수 증가량 $\Delta\Phi = \Delta\Phi(\Theta)$를 훨씬 작은 크기의 매개변수 집합 $\Theta$에 의해 인코딩하는 보다 매개변수 효율적인 접근법을 채택
        - GPT-3 175B의 경우, 0.01%만큼 작아질 수 있음

## 3. Aren’t Existing Solution Good Enough?

- 기존 접근 방식들
- 어댑터 레이어가 추론 지연을 초래
    - 어댑터 레이어 방식은 Transformer 블록에 두 개의 어댑터 레이어를 추가
        - 병목 차원을 작게 설정
        - 파라미터가 적기 때문에 하드웨어 병렬성을 활용하여 지연 시간 줄임
    - 어댑터 레이어는 순차적으로 처리
        - 일반적인 환경에서 모델 병렬성 없이 실행할 경우 지연이 발생
- 입력 레이어 최적화가 어려움
    - 입력 레이어 최적화 방식 중 하나인 '프리픽스 튜닝’
        - 튜닝이 어려움
            - 조정이 올바르게 이루어지지 않으면 모델의 전반적인 성능에 부정적인 영향
        - 학습 가능한 파라미터 변화 시, 성능이 크게 변화됨
            - = 파라미터를 더 많이 조정하거나 구조를 변경하는 것이 반드시 성능 개선으로 이어지지 않을 수 있음
        - 시퀀스 길이 제한
            - 프리픽스 튜닝은 입력 시퀀스의 일부를 프리픽스 토큰으로 대체 → 실제 입력 가능 데이터의 길이 줄어듦 →  성능 저하 초래 가능

## 4. Our Method

### 4.1 Low-Rank Parametrized Update Matrices

- LoRA는 신경망의 밀집층에서 수행되는 행렬 곱셈을 중심으로 작동
    - 기존의 가중치 행렬은 풀랭크(full-rank)
    - LoRA는 사전 훈련된 언어 모델이 이미 낮은 "내재 차원"(intrinsic dimension)을 가진다는 연구 결과에 기반
        - 가중치의 업데이트도 낮은 "내재 랭크"(intrinsic rank)
    - 가중치 행렬 $W_0$의 업데이트 $W_0$+BA 형태의 저차원 분해로 표현
        - B는 $R^{d}×r$ , A는 $R^{r}×k$
        - r은 min(d,k)보다 작거나 같음
        - 훈련 중에는 $W_0$ 고정, A와 B만 훈련 가능한 파라미터
        
- 풀 파인 튜닝의 일반화
    - 전통적인 풀 파인 튜닝은 모든 사전 훈련된 파라미터를 훈련
    - LoRA는 가중치 매트릭스의 업데이트가 반드시 풀랭크일 필요는 없다고 가정
    - LoRA의 랭크 $r$을 사전 훈련된 가중치 매트릭스의 랭크로 설정함으로써 풀 파인 튜닝의 표현력을 대략적으로 복구

- 추론 시 추가 지연 없음
    - $W_0$+BA가  $R^{d}×k$ 로 되어 있어서 다운스트림 작업 적용 시, $BA$가 아닌 $B^{'}A^{'}$ 를 더해 $W_0$ 복구 가능

### 4.2 Applying LoRA to transformer

- LoRA (Low-Rank Adaptation) 기법을 트랜스포머 모델에 적용
    - LoRA는 사전 훈련된 트랜스포머 기반 모델의 가중치를 부분적으로만 조정
    - 파라미터의 수를 대폭 줄이면서도 모델의 성능을 유지하거나 향상시키는 전략
- 저차원 업데이트 매트릭스 적용
    - 트랜스포머의 각 레이어에는 쿼리(query), 키(key), 값(value), 출력(output)의 가중치 매트릭스 $W_q, W_k, W_v, W_o$
    - LoRA는 이 중 일부 또는 전부에 저차원 업데이트 매트릭스 BA를 적용
        - self attention 레이어에서 $W_q, W_v$에만 LoRA를 적용 → 효율성 ↑, 메모리 사용 ↓
- 가중치 매트릭스 조정
    - 가중치 매트릭스 $W$는 $W_0+BA$로 업데이트
        - $W_0$는 원래 사전 훈련된 가중치, $BA$는 추가적인 저차원 업데이트 제공
        - B와 A는 각각 $d × r$과 $r × k$의 차원을 가짐
        - r은 업데이트의 랭크, d와 k는 원래 가중치 매트릭스의 차원
    - B와 A는 새롭게 도입보디는 저차원 rank 매트릭스

## 5. Empirical Experiments

- LoRA의 다운스트림 태스크 성능을 RoBERTa, DeBERTa, GPT-2, GPT-3 에 실험
- 자연어 이해 NLU, 생성 NLG 등 광범위한 작업 포함

### 5.1 Baselines

- 미세 조정(FT)
    - adaptation을 위한 일반적인 접근 방식
    - 모델은 사전 훈련된 가중치와 바이어스로 초기화되고 모든 모델 매개변수는 기울기 업데이트
    - 단순한 변형: 일부 레이어만 업데이트하고 다른 레이어는 동결
- Bias-only or BitFit
    - 모든 나머지를 동결하고 편향 벡터만 훈련
- Prefix-embedding tuning (PreEmbed)
    - 입력 토큰 사이에 특별한 토큰을 삽입
        - 특별한 토근: 학습 가능한 단어 임베딩
    - cf) "prefixing"에 집중하며, 이는 이러한 토큰을 프롬프트 앞에 추가하고, "infixing"은 프롬프트에 추가
    - 특정 토큰에 대한 단어 임베딩을 학습하는 방법. 기존의 입력 시퀀스에 프리픽스 토큰을 추가하고, 이 프리픽스에 대한 임베딩 값만을 업데이트하여 학습
- Prefix-layer tuning (PreLayer)
    - prefix-embedding tuning의 확장
    - 특정 토큰에 대한 단순한 워드 임베딩(또는 임베딩 레이어 이후의 활성화)뿐만 아니라 각 Transformer 레이어 이후의 활성화도 학습하는 방식
- Adapter tuning
    - 셀프 어텐션 모듈(MLP 모듈 포함)과 이후의 잔차 연결 residual connection 사이에 어댑터 레이어를 삽입
        - Residual connection은 딥러닝 모델, 특히 신경망에서 사용되는 기법으로, 주로 네트워크의 깊이가 깊어질 때 발생할 수 있는 기울기 소실 문제를 해결하기 위해 사용
        - 이 연결 방식에서는 입력을 직접 출력에 더하는 방식
        - 이전 레이어의 출력을 다음 레이어의 입력에 추가
        - 한 층의 출력이 $x$이고, 다음 층의 변환 결과가 $f(x)$일 때, residual connection을 사용하면 최종 출력은 $f(x)+x$
    - 어댑터 레이어
        - 비선형성이 있는 두 개의 완전 연결 레이어가 포함 (=AdapterH)
        - MLP 모듈 및 LayerNorm 후에만 어댑터 레이어를 적용하는 보다 효율적인 설계를 제안 (=AdapterL)
        - AdapterP, AdapterDrop

![image 1](https://github.com/user-attachments/assets/1b983abe-ca6b-4487-93ea-2541d57b5923)

- LoRA: 기존 가중치 행렬에 평행하게 학습 가능한 쌍의 랭크 분해 매트릭스를 추가

### 5.2 ROBERTA BASE/LARGE

- BERT (Devlin et al., 2019a)에서 제안된 사전 훈련 레시피를 최적화하고, 많은 추가 훈련 가능 매개변수를 도입하지 않으면서 후자의 작업 성능을 향상
- LoRA와 어댑터를 비교할 때 공정한 비교를 보장하기 위해 두 가지 중요한 변경을 수행
    1. 모든 작업에 대해 동일한 배치 크기를 사용하고 어댑터 기준에 맞추기 위해 시퀀스 길이를 128로 설정
    2. MRPC, RTE 및 STS-B에 대해 사전 훈련된 모델로 모델을 초기화하고, 전체 훈련된 기준점처럼 이미 MNLI에 적응된 모델을 사용하지 않음

### 5.3 **DEBERTA XXL**

- 훨씬 더 큰 규모로 훈련된 BERT의 최신 변형으로
- GLUE (Wang et al., 2019) 및 SuperGLUE (Wang et al., 2020)와 같은 벤치마크에서 좋은 성능

### 5.4 GPT-2 MEDIUM/LARGE

- E2E NLG 챌린지에 대한 결과만 제시
    - WebNLG (Gardent et al., 2017) 및 DART (Nan et al., 2020)에 대한 결과는 섹션 F.1을 참조
    - WebNLG는 주어진 구조화된 데이터를 자연어 텍스트로 변환하는 모델의 성능을 테스트하는 데 유용한 자원
    - DART의 입력 데이터는 ENTITY — RELATION — ENTITY 형식의 세트로 구성되
    - E2E NLG (End-to-End Natural Language Generation): 자연어 생성(NLG)의 한 접근 방식으로, 주어진 입력 데이터를 처리하여 최종적으로 자연어 문장을 생성하는 과정을 단일 모델이 수행하는 방식

### 5.5 SCALING UP TO GPT-3 175B

![image 2](https://github.com/user-attachments/assets/e51339f6-0edf-4ded-83a0-1de1e42cb821)

- LoRA는 세 가지 데이터 세트 모두에서 미세 조정 기준선과 동등하거나 초과하는 성능

## 6. Related Works

- Transformer Language Models
- Prompt Engineering and Fine-Tuning
- Parameter-Efficient Adaptation
    - 많은 연구자들은 신경망의 기존 레이어 사이에 어댑터 레이어를 삽입하는 방법을 제안
    - 기존의 어댑터 레이어(예: COMPACTER)는 크로네커 곱과 사전 정의된 가중치 공유 방식을 활용
    - LoRA와 같은 저차원 텐서 제품 기반 방법을 결합하면 매개변수 효율성을 더 높일 가능성 존재 (향후 연구 과제로 남겨둠)
    - PEFT (Parameter-Efficient Fine-Tuning)
        - 주로 대형 사전 학습 모델(PLM)에서 적은 수의 파라미터만 조정하거나 추가하여 특정 작업에 모델을 효율적으로 적응시키는 기술
        - **Adapter Layers**, **LoRA (Low-Rank Adaptation)**, 그리고 **COMPACTER** 같은 방법들이 모두 PEFT의 하위 기술로 분류
- Low-Rank Structures in Deep Learning
    - 기계 학습에서 저차원 구조는 매우 일반적이며, 많은 문제는 고유한 저차원 특성
        - 과대 파라미터화된 신경망을 사용하는 딥러닝 작업에서는 학습 후 신경망이 저차원 특성
    - 저차원 구조를 가진 개념 클래스에서 신경망이 고전적 학습 방법보다 우수하다는 것이 증명
    - 저차원 적응이 적대적 학습에도 유용하다는 결과
    - 저차원 적응 업데이트는 기존 문헌의 동기

## 7. Understanding the Low-Rank Updates

- **LoRA의 장점**
    - 저차원 구조는 하드웨어 요구 사항을 낮춰 여러 실험을 병렬로 실행 가능하게 하고, 사전 학습된 가중치와 업데이트된 가중치 간의 상관관계를 더 잘 해석할 수 있도록 도와줌.
    - GPT-3 175B를 대상으로 실험을 진행해, 작업 성능을 유지하면서 학습 가능한 매개변수를 최대 10,000배 줄이는 데 성공.
- **다운스트림 성능 극대화를 위한 질문**
    - LoRA를 적용해야 할 가중치 행렬
        - self attention 모듈의 가중치 중 **Wq**(쿼리)와 **Wv**(값)를 함께 조정하는 것이 가장 좋은 성능
        - 단일 가중치(Wq 또는 Wk)를 조정하는 경우 성능이 크게 감소
        - 랭크(r=4)로 두 개 이상의 가중치를 조정하는 것이 더 효율적
    - 최적의 랭크(r)
        - 랭크가 작아도 (r=1 또는 4) LoRA는 높은 성능을 유지
        - Wq, Wv와 같은 여러 가중치를 함께 조정하면 낮은 랭크에서도 충분한 정보를 학습 가능.
        - 과하게 큰 경우 성능향상 기여 X
    - ∆W와 W의 관계
        - ∆W는 W에서 강조되지 않은 중요한 특징을 강조
        - 다운스트림 작업 성능 향상 기여