# Visual Instruction Tuning

날짜: 2024년 11월 1일

[https://arxiv.org/pdf/2304.08485](https://arxiv.org/pdf/2304.08485)



## 1. 서론

- 배경
    - 시각적 신호를 언어 의미로 매핑 필요
    - 대형 언어 모델의 발전과 성능
- 기여점
    - Multimodal instruction-following data
    - Large multimodal models
    - Multimodal instruction-following benchmark
    - Open-source



## 2. Related Work

- Multimodal Instruction-following Agents
    - 컴퓨터 비전에서의 instruction following:
        - 특정 연구 주제마다 엔드투엔드 훈련 모델 구축
            - 엔드투엔드
                - 시스템이나 모델이 입력에서 출력까지의 모든 단계를 하나의 통합된 과정으로 처리
                - 데이터를 입력하면 최종 결과를 얻기까지 중간 단계에서 별도의 분리된 처리가 없이 바로 출력을 생성하는 방식
        - LangChain/LLM을 통해 다양한 모델을 조정하는 시스템
            - Visual ChatGPT
- Instruction Tuning
    - LLM에서 Instruction Tuning 방법을 통해 LLM의 성능 향상, 제로샷 및 few shot 일반화 능력 향상
    - 이미지-텍스트 쌍을 기반으로 훈련된 다른 LMM: 비전-언어 instruction에는 부족함
    - Visual Instruction Tuning ≠ Visual Prompt Tuning
        - Visual Instruction Tuning 모델의 지침 따르기 능력을 향상시키는 것을 목표
        - Visual Prompt Tuning 모델 적응에서의 파라미터 효율성을 향상시키는 것을 목표



## 3. GPT-assisted Visual Instruction Data Generation

- 이미지-쌍 데이터를 기반으로 ChatGPT/GPT-4를 사용하여 멀티모달 Instruction following 데이터 수집 제안
- LAION 데이터: 이미지와 캡션 존재

**기본 아이디어**

- 캡션이 답(이미지에 대한 설명)이 되도록 질문 집합 생성
    - 다양성과 심층적인 측면에서 부족함

**수정된 아이디어**

- Symbolic Representation; 두 가지 형식을 통해 이미지를 LLM(대규모 언어 모델)이 인식할 수 있는 시퀀스로 변환 (이미지 넣은게 아니라 아래 형식으로 자연어만 넣음)
    - 캡션: 시각적인 장면을 다양한 관점에서 설명하는 문장입니다.
    - 바운딩 박스: 장면 내의 객체를 지역화하며, 각 박스는 객체의 개념과 공간적 위치를 인코딩합니다.
- instruction-following 데이터셋: 세 가지 유형으로 생성
    - 대화형: 이미지에 대해 질문을 하고 그에 대해 답변하는 형식으로 구성
    - 자세한 설명: 이미지에 대한 풍부하고 포괄적인 설명을 목표로 하는 질문 목록을 생성
    - 복합적 추론: 이미지와 관련된 논리적 가이드를 필요로 하는 질문들로 구성
- 158k의 언어-이미지 지침 샘플 수집

![image](https://github.com/user-attachments/assets/a9520e4d-18ae-426a-a324-f17543623dc3)



## 4. Visual Instruction Tuning

### 4.1 Architecture

- 비전 인코더: CLIP
    - 이미지와 텍스트 쌍을 함께 학습시켜, 이미지와 관련된 텍스트의 의미를 이해할 수 있도록 훈련
    - Zv는 특정 이미지가 지닌 시각적 정보의 추상적 표현, 그 이미지와 관련된 텍스트 설명 또는 개념과의 유사성을 계산하는 데 사용
    - Zv는 텍스트와 관련된 시각적 특징을 포함
- 프로젝션 매트릭스 W (선형 레이어)
    - 이미지를 단어 임베딩 공간에 연결하기 위해 간단한 선형 레이어를 고려
    - Zv → Hv로 변환
    - 크로스 어텐션, BLIP-2의 Q-former와 같은 방법

![image 1](https://github.com/user-attachments/assets/c8aa1757-778e-4d49-9584-dce7b62c266c)


### 4.2 Training

- two-stage instruction-tuning

1. **Pre-training for Feature Alignment**
- 시각 인코더와 LLM 가중치를 모두 고정, 학습 가능한 매개변수 $\theta = W$ (투영 매트릭스)로만 학습
    - 일종의 이미지 특징을 LLM의 단어 임베딩과 정렬시키기 위한 적합한 비주얼 토크나이저를 학습

1. **Fine-tuning End-to-End**
- 항상 시각 인코더 가중치를 고정, LLaVA에서 투영 레이어와 LLM의 사전 훈련된 가중치를 계속 업데이트
- 학습하는 매개변수 $\theta = \{W, \phi\}$
- 특정 사용 사례 시나리오 고려
    1. 멀티모달 챗봇: 158K 언어-이미지 instruction following 데이터로 파인튜닝 (다중 턴, 단일 턴 포함)
    2. 과학 QA: ScienceQA 벤치마크
        - 각 질문은 자연어 또는 이미지 형태로 컨텍스트를 제공



## 5. Experiments

- LaVA의 성능: 멀티모달 챗봇, ScienceQA 데이터셋 사용하여 평가

### 5.1 Multimodal Chatbot

- GPT-4 논문의 예제를 사용
- 비교를 위해,
    - 논문에서 다중 모달 GPT-4의 프롬프트와 응답을 인용
    - BLIP-2 및 OpenFlamingo 모델 체크포인트에 질의를 통해 응답 받음
- 결과
    
    ![image 2](https://github.com/user-attachments/assets/44f8274c-d0c4-413a-a0b1-cd0d8e9fcbae)
    
    - LLaVA
        - 단순히 장면을 설명하는 대신 보다 포괄적인 응답을 제공
        - 이미지의 비정상적인 측면을 파악
        - 소규모 데이터셋으로 학습했지만, GPT-4와 유사한 추론 결과
        - 도메인 외의 이미지이지만, 여전히 장면 이해, 질문에 대한 적절한 응답 제공 가능
    - BLIP-2와 OpenFlamingo
        - 사용자 instruction을 따르기보다 이미지 설명하는데 초점

- **정량적 평가**
    - 평가 데이터셋 구성: image, ground-truth textual descriptions, and question
    - 후보 모델(예: LLaVA): 질문과 이미지를 기반으로 답변을 예측
    - 비교 모델(GPT-4): 질문과 정답 텍스트 설명을 기반으로 텍스트 전용 GPT-4를 사용하여 참조 예측을 생성
    - 심사관: 보조자의 응답을 유용성, 관련성, 정확성 및 세부 수준에 따라 평가하고 1에서 10까지의 척도로 전체 점수를 부여


- **LLaVA-Bench (COCO)**
    - COCO-Val-2014에서 무작위로 30개의 이미지를 선택
    - 각 이미지에 대해 대화, 세부 설명, 복잡한 추론의 세 가지 유형의 질문을 생성하여 총 90개의 질문
    - 결과
        - instruction tuning 이 유의미하게 향상됨
        - 세부 설명 및 복잡한 추론 질문을 추가 학습하는 것이 성능 향상에 도움이 됨
        - 다 사용하는게 최상의 성능

![image 3](https://github.com/user-attachments/assets/254bc104-ec50-4008-80cb-1548c33a3489)

- **LLaVA-Bench (In-the-Wild)**
    - 다양한 24개의 이미지를 수집하고 총 60개의 질문을 포함
    - 도전적이며 모델의 약점을 드러내기 위해 설계: 광범위한 지식 범위와 다국어 이해 능력을 요구
    - 결과
        - Instruction Tuning 으로 인해 LLaVA는 BLIP-2에 비해 29%, OpenFlamingo에 비해 48% 더 나은 성능을 달성

![image 4](https://github.com/user-attachments/assets/34e568f4-d778-445c-b139-8437eeafe374)


### 5.2 SciencQA

- 21,000개의 다중 모드 선택 질문을 포함하고 있으며, 3개의 주제, 26개의 주제, 127개의 카테고리 및 379개의 기술에 걸쳐 다양한 도메인
- 우리 모델과 GPT-4의 결과를 결합하기 위해 두 가지 방안을 고려
    - (i) GPT-4 보완. GPT-4가 답변을 제공하지 못할 때마다 우리 방법의 예측을 사용: 90.97% 정확도 = 우리 방법만 사용하는 것과 거의 동일
    - (ii) GPT-4를 판별자로 사용: GPT-4와 LLaVA가 서로 다른 답변을 생성할 때마다, 우리는 질문과 두 가지 결과를 기반으로 GPT-4에게 다시 물어보아 자신의 최종 답변을 제공하도록 요청 → 92.53%라는 새로운 SoTA 정확도를 달성
- Ablations
    1. 시각적 특징: 
        - 우리는 CLIP 비전 인코더의 마지막 층 특징을 사용해 보았고, 이는 89.96%를 산출하며 이전 층의 특징보다 0.96% 낮습니다
        - CLIP의 마지막 층 특징이 전체적이고 추상적인 이미지 속성에 더 집중할 수 있기 때문이라고 가설
    2. Chain-Of-Thought: 최종 성능에는 상대적으로 적은 기여를 한다고 결론
    3. Pretraining: 사전 훈련 단계의 중요성
    4. 모델 크기: 규모 중요성 입증