# Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent

날짜: 2025년 1월 2일

- [https://arxiv.org/pdf/2411.02937](https://arxiv.org/pdf/2411.02937)
- ICLR 2025 Under Review

휴리스틱 방식은 고정된 단계로 진행되는 기존의 검색 전략을 의미하고, OmniSearch는 이를 개선하기 위해 설계된 에이전트 기반의 동적 검색 시스템임



## Abstract

- mRAG (Multimodal Retrieval Augmented Generation): 다중 모달 대형 언어 모델(MLLMs)에 내재된 "환각" 문제를 완화하는 데 중요한 역할
- 기존의 휴리스틱 mRAG 두 가지 문제: (1) 비적응적 검색 쿼리, (2) 과부하된 검색 쿼리
- 데이터셋의 간격을 해소하기 위해, Dyn-VQA 데이터셋을 동적인 질문이 필요한 세 가지 유형의 "동적" 질문들로 구성
    - (1) 빠르게 변화하는 답변을 가진 질문
    - (2) 다중 모달 지식을 요구하는 질문.
    - (3) 다단계 질문.
- 다중 모달 검색을 위한 첫 번째 자기 적응형 계획 에이전트인 OmniSearch를 추가로 제안

## 1. Introduction

- **mRAG**
    - 외부 출처에서 보다 포괄적이고 정확하며 최신의 지식을 제공하는 것을 목표
    - "환각" 문제 완화 주요 기술

- **기존의 휴리스틱 mRAG 방법**
    - 일반적으로 모든 모달리티를 하나의 주요 모달리티(보통 텍스트)로 통합한 고정된 검색 프로세스를 미리 정의하여 단일 시간에 대해 검색을 수행
    - 두 가지 문제
        - (1) 비적응적 검색 쿼리:
            - 휴리스틱 mRAG 방법의 고정된 검색 프로세스 및 쿼리 구조를 지칭
            - 예) "Cillian Murphy의 최신 영화는 무엇인가요?"라는 질문에 대해 관련 영화는 반환되지만, 구체적으로 가장 최신의 영화를 식별 불가
        - (2) 과부하된 검색 쿼리:
            - 입력 질문과 이미지의 객체에 대한 텍스트 설명을 연결하여 단일 쿼리를 형식화하는 것만을 의미
            - 단일 쿼리: 여러 검색 측면을 포함하여 모호한 검색과 관심 없는 지식의 유입을 초래
            - 예) "Ling Jia와 Teng Shen 중 누가 더 많은 수익을 냈나요?"라는 질문에서, 두 배우의 박스오피스 정보를 각각 정확히 제공 불가
        - 유연 X
    
- **Dyn-VQA**
    - 기존 VQA
        - mRAG 벤치마크로 사용되지만, 강직성 문제 반영 X
        - 두 단계 내의 텍스트 지식만 요구
    - 복잡한 다중 모달 지식 검색이 필요한 1,452개의 동적 질문으로 구성
    - 세 가지 유형의 동적 질문
        - (1) 빠르게 변하는 답변이 있는 질문: 현재 검색된 콘텐츠에서 피드백을 기반으로 추가 검색을 유연하게 계획
        - (2) 다중 모달 지식이 필요한 질문: mRAG 방법이 다양한 모달리티에서 지식을 검색하기 위해 맞춤형 검색 API를 사용해야 함을 요구
        - (3) 다단계 질문: 솔루션을 위해 다양한 추론 단계를 요구, 다양한 검색 단계를 수행 필요

- **OmniSearch**
    - Dyn-VQA를 MLLM으로 평가
        - Dyn-VQA의 동적 질문에 대해 충분하고 정확하게 관련 지식을 제공하는 데 어려움
    - OmniSearch: 다중 모달 검색을 위한 자기 적응형 계획 에이전트
        - 다이나믹하게 복합적인 다중 모달 질문을 하위 질문 체인으로 분해하여 검색 작업을 수행
        - 각 단계에서 OmniSearch는 질문 해결 상태와 검색된 내용에 따라 다음 작업을 유연하게 조정
        - 검색된 내용에 대한 이해를 심화하거나 현재 하위 질문에 대한 검색 방법을 수정하거나 다음 하위 질문을 제안하는 등 다양한 목적
        - 임의의 MLLM과 협력하여 복잡하면서도 동적인 질문을 처리할 수 있는 RAG 모듈로 작동할 수 있다는 점에서 주목할 만함
        - OmniSearch의 두 가지 서로 다른 버전이 각각 클로즈드 소스 GPT-4V (Achiam et al., 2023) 및 오픈 소스 Qwen-VL-Chat (Bai et al., 2023a)를 기반으로 개발

- **기여점**
    - Dyn-VQA 데이터셋을 제안
    - MLLM과 비교하여 동적 질문에 대한 충분하고 관련성 있는 지식을 제공하는 데  한계가 있음을 입증
    - OmniSearch를 제안
    - OmniSearch의 효과를 입증

## 2. Related Works

### 2.1 다중 모달 검색 증강 생성

- mRAG 방법
    - 첫 번째 방법
        - 직접 이미지 표현을 위한 시각적 인코딩 모델을 사용하고, 그런 다음 가장 높은 특성 유사성을 가진 지식을 검색
        - KAT (Gui et al., 2022), Revive (Lin et al., 2022): CLIP의 이미지 인코더를 검색에 사용
    - 두 번째 방법
        - 입력 이미지를 기존 도구를 사용해 텍스트 표현으로 변환한 다음 텍스트 검색을 수행
        - RA-VQA (Lin & Byrne, 2022) , RA-VQA-v2 (Lin et al., 2024): 먼저 기존의 객체 탐지 및 이미지 캡션 모델을 사용해 이미지를 텍스트로 변환한 후 밀집 문서 검색을 수행하여 관련 텍스트 문서를 가지고 옴

- OmniSearch의 목적
    - MLLM에 적절하고 정확한 지식을 제공하기 위한 기존의 작업과 일치
    - 세 가지 측면에서 차별화
        - (1) OmniSearch는 각 질문에 대해 다양한 검색 도구를 사용하여 여러 검색 작업을 계획해 각 모달리티의 부족한 지식을 보완
        - (2) OmniSearch는 검색된 내용을 기준으로 후속 검색 작업을 동적으로 조정하며, 입력 질문과 이미지만으로 쿼리를 생성하는 방법과는 차별화
        - (3) OmniSearch의 검색 범위는 전체 Internet에까지 확장하여 복잡하면서도 더 포괄적인 지식을 제공

## 3. Dyn-VQA Dataset

### 3.1 Dyn-VQA Construction

- 데이터셋 품질을 보장하기 위해 데이터셋의 전체 목표를 전문 AI 연구자인 주석 작성자에게 설명
- 초기 주석에서는 이 과도하게 복잡한 단일 단계 전략이 상당히 비현실적
    - 주석자는 종종 이미지를 먼저 검색한 다음 다양한 기준(예: 속도 변화, 추론 단계 등)을 염두에 두고 해당 질문을 고안하는 데 많은 노력을 들여야 하는 딜레마
- 전략을 최적화하고 세 단계로 구축
    - 1단계. 텍스트 질문 작성
        - 3차원 스키마를 사용하여 분류
        - 주석자는 텍스트 질문을 작성,
        - 이를 답변 업데이트 빈도(빠름, 느림, 전혀 없음),
        - 외부 시각적 지식 필요 여부(예, 아니오)
        - 추론 단계(≤ 2-hop, > 2-hop)
    - 2단계. 멀티모달 재작성
        - 주석자는 1단계에서 작성된 텍스트 질문을 시각적 개념을 공통 참조로 교체(예: "코비 브라이언트"를 "이 선수"로)
        - 수정된 질문을 구글에서 찾은 관련 이미지와 쌍으로 만들어 멀티모달 질문으로 변환
        - Wikipedia 및 Baidu Encyclopedia와 같은 일반적으로 사용되는 사전 학습 코퍼스에서 가져온 이미지는 금지
    - 3단계. 중국어-영어 번역.
        - Dyn-VQA의 중국어 및 영어 부분을 나란히 배치하여 언어 비교 (Google Translate API를 사용하여 서로 번역)
        - 고유 명사를 보장하기 위해 수동 점검 및 수정이 수행

### 3.2 Dyn-VQA Analysis

![image](https://github.com/user-attachments/assets/eea80a5d-84ed-462a-856d-081d285250b1)

- mRAG 시스템의 효능을 평가하기 위해 특별히 제안된 첫 번째 데이터 세트
    - 9개의 도메인에 걸쳐 약 1500개의 질문을 포함
    - 복잡한 동적 검색이 필요한 3가지 유형의 질문(빠르게 변화하는 답변을 요구하는 질문, 멀티모달 지식이 요구되는 질문, 멀티홉 질문)을 포함

- Dyn-VQA와 다른 지식 탐색 VQA 데이터 세트 간의 비교
    
    ![image 1](https://github.com/user-attachments/assets/fb3d098f-d82a-4827-96f1-afce46f073ec)
    
    - 다른 데이터 세트: 질문 해결에 외부 지식의 필요성을 강조, 그들 관련 지식은 일반적으로 한 가지 범주에만 해당
    - Dyn-VQA: 각 질문은 실제 세계에서 유래, 더 넓고 이질적인 지식 유형 범위를 포함하고, 더 개방형 답변 스타일을 특징
        - 사람이 세심하게 큐레이션
        - Dyn-VQA는 규모 면에서는 다른 데이터셋과 일치하지 않을 수 있지만 품질, 난이도, 각 사례의 비용 측면에서는 훨씬 뛰어남

- **데이터셋 난이도**
    - Dyn-VQA의 질문은 더 복잡한 외부 지식을 요구, 검색 과정은 고정적이지 않음
    - → 본질적인 동적 특성은 자연스럽게 그 어려움을 보장
    - 표3) 기존 VQA 데이터셋
        
![image 2](https://github.com/user-attachments/assets/efe3dae9-df73-40c1-ab2a-6b8a736312af)
        
- 일반적으로 두 번의 추론 단계 내에서 해결
- 이미지 내 객체에 대한 텍스트 설명 외에 추가적인 시각적 지식이 필요하지 않기 때문에 텍스트 쿼리를 통한 이미지 검색 수행X
  

## 4. Retrieval Baselines and OmniSearch

- 동적 질문을 해결하기 위해 여러 휴리스틱 mRAG 기준을 설정하고 OmniSearch를 개발
- 모든 방법의 검색 도구는 Google 기반으로, 웹 검색(텍스트 쿼리를 통한 텍스트 웹 콘텐츠 검색), 입력 이미지에 의한 이미지 검색, 텍스트 쿼리에 의한 이미지 검색이 포함

### 4.1 Baselines

- **Single-hop heuristic mRAG baselines.**
    - 첫 번째 휴리스틱 기준: 입력 이미지로 이미지를 검색
        - 유사한 이미지를 제공하고 설명 캡션을 함께 보여줌
        - 입력 이미지에 묘사된 객체에 대한 시각적 지식으로 MLLMs를 보강
    - 두 번째 휴리스틱 기준: 입력 텍스트 질문으로 웹 검색을 수행하고 MLLMs에 상위 k 검색된 콘텐츠를 보조 지식으로 제공
    - 입력 질문의 부분 모달리티만 검색 쿼리로 활용하므로 정확한 지원 정보를 제공하지 못할 수 있음
    - 그럼에도 불구하고 이러한 기준은 단일/교차 모달리티 검색의 이점을 탐색하기 위해 여전히 설정

- **Two-hop heuristic mRAG baselines.**
    - 일반적으로 기존 mRAG 방법은 두 가지 주요 단계로 일반화
        - 입력 이미지의 시각적 개념을 텍스트로 변환
        - 얻어진 텍스트를 입력 질문과 결합하여 검색 쿼리로 관련 텍스트 지식을 검색하는 것
    - 첫 번째 단계에서는 입력 이미지로 검색된 상위 1 이미지의 캡션과 이미지 캡션 모델의 출력을 각각 입력 이미지의 텍스트 표현으로 활용
    - 그 후, 웹 검색 API를 사용하여 인터넷에서 관련 지식을 추출
    - 두 단계 mRAG 방법은 질문과 더 밀접하게 관련된 지식을 보다 정확하게 제공
    - 그러나 여전히 고정된 검색 프로세스의 제한에 직면

- **Estimated Upper Bound.**
    - golden query" (이상적인 검색 쿼리)의 주석을 통해 mRAG의 상한을 추정
    - 최종 검색 단계로 단순화한 형태
    - 질문이 외부 시각적 지식을 요구하는지 여부에 따라 웹 검색 또는 이미지 검색 API가 호출됨
    - 기존 **휴리스틱 mRAG** 방법론이 얼마나 이상적인 성능에 가까운지 비교하고, 그 한계를 분석하기 위함
    - Upper Bound는 실제 구현에서의 성능과 비교하여 현재 접근 방식의 **효율성**과 **개선 가능성**을 평가하는 데 사용

### 4.2 OminiSearch Framework

![image 3](https://github.com/user-attachments/assets/ea70b589-983c-457d-b3e4-9a7e1178667b)

- 휴리스틱 mRAG은 경직된 검색 프로세스에서 발생하는 문제에 직면
- 보다 유연한 모델이 필요
- 기본 아이디어는 복잡한 질문을 일련의 해결 작업으로 점진적으로 분해하는 인간의 방식을 모방하는 것
- **Planning Agent.**:서브 질문을 공식화하고 실제 세계 피드백(즉, 검색된 콘텐츠 또는 해결자 출력)을 바탕으로 이후 검색 작업을 계획하는 핵심 모듈
- **Retriever**: 실제 검색 작업 실행
- **Sub-question Solver**: 검색된 콘텐츠를 기반으로 서브 질문에 대한 피드백을 생성하고 이를 계획자에게 업데이트

- **Planning Agent.**
    - 각 계획된 작업은 네 가지 핵심 부분으로 구성: 자기 사고 <ST>, 서브 질문 <SQ>, 검색 API <R>, API 쿼리 <Q>
    - 각 단계에서 계획 에이전트는 주어진 질문과 실세계 피드백을 자기 사고에서 이해한 후, 해결할 후속 서브 질문을 신중하게 결정
    - 이와 동시에 필요로 하는 지식 유형에 따라 다양한 검색 API와 쿼리를 호출
    - 질문 해결 중 인간의 인지 과정과 유사한 방식으로, 계획 에이전트는 다음과 같은 다양한 잠재적 행동을 자율적으로 생성:
        - 검색된 콘텐츠의 모호하거나 상충하는 부분을 명확하게 하기 위한 추가 질문 제시
        - 보조 지식을 얻기 위해 검색 쿼리 수정
        - 서브 질문의 문구 변경
        - 현재 서브 질문에 대한 응답 검증
        - 다음 서브 질문 제시
        - 최종 답변 결론 도출 등

- **Retriever**
    - 실제 검색 작업을 수행
    - 웹 검색, 텍스트 및 이미지 검색을 포함한 이미지 검색이 포함
    - 향후 더 많은 검색 도구를 고려 가능

- **Sub-question Solver**
    - 검색된 콘텐츠를 요약하고 이에 따라 서브 질문에 응답하려고 노력
    - Solver가 생성한 피드백은 에이전트에 제공
    - Solver는 임의의 MLLM 또는 심지어 Planning Agent 자체와 호환, 검색된 정보를 직접 계획자에게 반환 가능

- OmniSearch는 최종 답변을 출력하는 데 충분한 지식을 검색했다고 믿을 때까지 반복 작업을 수행
- 각각의 MLLM에 따라 두 가지 버전의 OmniSearch를 훈련
    - 독점적 GPT-4V와 오픈 소스 Qwen-VL-Chat.
    - GPT-4V의 경우, 동적 계획 및 의사 결정 능력을 자극하기 위해 프롬프트 엔지니어링을 사용
    - Qwen-VL-Chat의 계획 및 검색 API 활용 능력을 촉진하기 위해, GPT-4V의 합성 데이터와 기존 Infoseek 데이터셋을 활용하여 검색 API 훈련 데이터셋을 구성
- MLLM을 다중 대화 모드에서 훈련
- 훈련된 에이전트의 일반 대화 능력을 유지하기 위해 일반 지침 데이터도 활용

- **OmniSearch의 제안**
    - Chain-of-Thought (CoT) Wei et al. (2022)에서 영감을 받았지만 본질적으로 다름
    - 차이: 도구를 활용하고 환경과 상호작용하며 환경에 반응하는 능력

## 5. Experiments

### 5.1 Experimental Settings

- **Backbone MLLMs for heuristic mRAGs.**
    - 여러 고급 MLLM을 백본 모델로 선택하여 효율성을 입증
    - Qwen-VL-7B-Chat,
    - GPT-4V, Qwen-VL-Max3 폐쇄형 모델
    - Qwen-7B-Chat 텍스트 전용 LLM

- **평가 메트릭**
    - F1-Recall이 평가 메트릭으로 사용
    - 모델이 생성한 응답과 정답 간의 공통 토큰 비율을 계산
        - 생성된 텍스트와 정답 텍스트를 토큰 목록으로 나누고,
        - 모델이 생성한 토큰이 정답 토큰 목록에 속하는 비율을 계산

### 5.2 Main Results

![image 4](https://github.com/user-attachments/assets/98433123-efaa-423b-9470-f907440a1572)

- **(1) 우리의 OmniSearch (GPT-4V)**
    - 최첨단 MLLM과 휴리스틱 mRAG를 포함한 다른 모델뿐만 아니라 상업적 생성 검색 엔진을 상당히 능가
    - Qwen-VL-Chat 기반의 OmniSearch조차도 두 단계 휴리스틱 mRAG를 장착한 훨씬 더 큰 GPT-4V를 초과
    - 두 가지 측면에 기인
        - 첫째, OmniSearch는 복잡한 질문을 일련의 하위 질문으로 분해하여 단일 단계에서 검색 부담 감소
        - 둘째, 검색된 내용과 하위 질문을 재검토하여 하위 답변의 정확성을 보장하고 오류 전파의 위험을 완화

- **(2) 전반적인 성능**
    - OmniSearch는 golden query를 통해 인간, GPT-4V와 밀접하게 유사하여 그 우수한 능력
    - 세 가지 가장 도전적인 하위 범주(급변하는, >2-단계, 외부 시각 지식 요구)에 속한 질문
        - OmniSearch와 인간 간에는 여전히 상당한 간극
        - 현실 세계 질문을 위한 에이전트 기반 mRAG의 개선이 필요

- **(3) 50% 이상의 성과를 달성했음에도 불구하고, Dyn-VQA는 AI 시스템과 인간 모두에게 상당한 도전 과제**
    - 다단계 검색이나 추가 시각 지식을 요구하는 질문의 경우, 모든 모델이 같은 분류 체계 내의 다른 질문과 비교해 일관되게 성능이 저하되는 것으로 관찰
    - 서로 다른 답변 업데이트 빈도를 가진 질문의 경우, 모델 성능의 변동성 높음
    - 빠르게 변하는 지식이 필요한 질문이 가장 해결하기 어려운 과제, 이러한 지식은 MLLM에 의해 내재화 불가

- **(4) 두 단계 휴리스틱 mRAG의 경우**
    - 이미지 캡션 모델을 활용해 시각적 개념을 변환하는 것이 원래 MLLM에 더 많은 이득
    - 그러나 추가 시각 지식이 필요하지 않은 질문의 경우 이 이점이 reverse
    - 이들 질문의 대부분이 2단계 (74%)로, 이미지 자체에 제시된 개념을 넘는 시각적 지식이 필요하지 않기 때문
    - 이미지 캡션 모델의 보충 정보는 모델에 실질적인 이익 X

- **(5) 상업적 생성 검색 엔진은 일반적으로 Dyn-VQA에서 성능이 저조**
    - 가장 성능이 좋은 엔진인 Gemini조차도 두 단계 mRAG를 장착한 GPT-4V의 성능에만 해당
    - 추가 사례 분석 결과
        - 생성 검색 엔진이 필수적인 기초 능력이 부족
        - 질문 내의 “그것(it)”을 이미지의 객체와 연관시키지 못하며, 다중 모드 정보를 효과적으로 통합 불가

- **(6) Qwen-7B-Chat과 Qwen-VL-Chat을 비교**
    - mRAG가 장착되면 모델 간의 성능 격차 감소
    - mRAG가 순수 텍스트 LLM이 다중 모드 문제를 해결하는 데 도움

### 5.3 Analysis Exeriments on OmniSearch

![image 5](https://github.com/user-attachments/assets/3a26d251-29d0-429d-9f1e-697b7afe9dbd)

- 각 모델의 Sub-question Solver 역할이 전체 성능에 미치는 영향
    - 대규모 모델 사용 효과
        - Qwen-VL-Chat 기반 OmniSearch에서 Sub-question Solver 로 GPT-4V를 사용하면 성능이 크게 향상. 모델 크기를 확장할수록 성능이 좋아진다는 스케일링 법칙이 여전히 유효
        - Sub-question Solver을 Qwen-VL-Chat으로 교체하면 성능이 감소하지만,  Qwen-VL-Chat 기반 mRAG(2단계 휴리스틱)보다 우수한 성능
    - 복합적 호출 전략
        - 다중 모드 컨텍스트를 포함하는 서브 질문에는 GPT-4V를 활용, 순수한 텍스트 컨텍스트와 관련된 질문에는 GPT-4를 사용
        - 텍스트 전용에는 GPT-4가 낫다
    - OmniSearch 자체 성능
        - Qwen-VL-Chat 기반의 OmniSearch의 Sub-question Solver를 Qwen-VL-Chat으로 대체
        - 서브 질문 해결자로 OmniSearch를 사용하는 것이 질문 해결 능력을 향상
- Sub-question Solver가 토큰 및 비용에 미치는 영향
    - 비용 대비 성능:
        - GPT-4V에서 Qwen-VL-Chat으로 교체 시 성능은 약간 감소(7.9%)하지만, 비용은 절반으로 줄어 확장성 입증.
    - 검색 전략의 중요성:
        - 하위 질문 추론보다 검색 계획 전략이 더 중요하며, 대규모 모델을 검색 계획에 우선적으로 사용해야 효율적.

![image 6](https://github.com/user-attachments/assets/1841d1dd-c5e5-4c2c-9939-c3bac5f3a54d)

## 6. Analysis Experiments on Dyn-VQA Dataset

### 6.1 Performance comparison on different domains

- mRAG 방법을 사용하면 원래 모델의 성능 범위가 확장
- 교통 도메인:
    - Qwen-VL-Chat 기반 OmniSearch가 GPT-4V 기반보다 더 나은 성능을 보이는 사례가 있음. 교통 도메인의 데이터가 10개의 VQA 인스턴스만 포함, 대부분 2단계 질문으로 구성된 롱테일 특성 때문
    - GPT-4V 기반 OmniSearch는 필요한 정보를 검색한 후에도 과도하게 추가 정보를 수집하려 해, 정답이 많은 검색 결과 속에 묻히는 경향
- OmniSearch의 강인성을 개선 필요

### 6.2 Prediction Overlap

- 질문 정확도 분석
    - Dyn-VQA에서 모든 모델이 올바르게 답한 질문은 없었으며, 질문의 31%는 어떤 모델에서도 정확한 예측을 받지 못함.
    - Qwen-VL-Max와 GPT-4V가 가장 높은 성능을 보였지만, 여전히 올바르게 답한 질문의 중첩률은 약 60% 수준.
- 모델의 성능 차이를 극복하기 위해 향후 연구로 앙상블 기반 및 자기 일관성 기반 접근 방식 을 제안

## 7. 결론

- 다중 모드 검색 증강 생성(mRAG)을 연구
- 기존의 휴리스틱 mRAG가 일반적으로 고정된 검색 프로세스를 미리 정의하여 두 가지 문제를 야기한다고 주장
    - (1) 비적응형 검색 쿼리. (2) 과부하 검색 쿼리
    - 이러한 경직성 문제는 현재의 지식 탐색 시각 질문 응답(VQA) 데이터셋에서 해결 불가
    - 복합 지식 검색 전략을 필요로 하는 세 가지 유형의 "동적" 질문으로 구성된 Dyn-VQA 데이터셋을 생성
- OmniSearch는 다중 모달 검색을 위한 최초의 자기 적응형 계획 에이전트로 제안
