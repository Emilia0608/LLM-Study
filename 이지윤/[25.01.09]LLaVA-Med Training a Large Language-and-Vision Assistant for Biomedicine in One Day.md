# LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day

날짜: 2025년 1월 9일

- NeurIPS 2023
- [https://arxiv.org/pdf/2306.00890](https://arxiv.org/pdf/2306.00890)
- 마이크로소프트에서 작성한 논문
- LLaVA 깃허브에서 연혁(history) 보면 LLaVA Med 도 언급되어 있음. 아마 첫 도메인 적용 예시여서 그런듯..? 아니면 협력?
- LLaVA NeurIPS 2023 Oral 로 발표됐는데 LLaVA-Med는 NeurIPS 2023 Spotlight로 발표됨..

## Abstract

- biomedical images
    - 멀티모달 대화형 AI
    - 일반 도메인의 비전-언어 모델은 생물 의학 이미지를 이해하고 대화하는 데 있어 여전히 정교함이 부족
- 아이디어
    - PubMed Central에서 추출한 대규모의 폭넓은 생물 의학 그림-캡션 데이터셋을 활용
    - GPT-4를 사용하여 캡션으로부터 개방형 지침을 자가 학습하는 데이터
    - 새로운 교육 커리큘럼 학습 방법을 사용하여 대형 일반 도메인 비전-언어 모델을 미세 조정하는 것
        - 모델은 먼저 그림-캡션 쌍을 그대로 사용하여 생물 의학 어휘를 정렬하는 방법을 배우고
        - GPT-4에서 생성된 지침 기반 데이터를 활용하여 개방형 대화 의미론을 습득하여 일반인이 생물 의학 지식을 점진적으로 습득
- 세 가지 표준 생물 의학 시각 질문 응답 데이터셋에서 LLaVA-Med를 미세 조정

## 1. 서론

- 일반 도메인에서의 성능에도 불구, LMM은 **생물 의학 시나리오에서는 성능 부족**
    - 생물 의학 이미지-텍스트 쌍과 일반 웹 콘텐츠의 갭
    - 생물 의학 질문에 대한 응답을 생성하는 것은 종종 부정확한 결과나 완전한 환각을 초래
    - 생물 의학 시각 질문 답변(VQA)
        - 기존 방법들은 일반적으로 문제를 분류 문제로 설정 (예: 훈련 세트에서 관찰된 다양한 답변 중에서)
        - 개방형 지침을 따르는 데 적합하지 않음

- **Large Language and Vision Assistant for BioMedicine (LLaVA-Med)**
    - 도메인 특화 프리트레이닝은 생물 의학 자연어 처리(NLP) 응용 분야에서 효과적임이 입증
    - PMC-15M이라는 대규모 생물 의학 비전-언어(VL) 학습이 가능
    - PMC-15M의 이미지-텍스트 쌍을 사용하여 다양한 생물 의학 다중 모달 지침 데이터를 생성
    - 새로운 커리큘럼 학습 방법을 사용하여 대규모 생물 의학 분야 VL 모델을 미세 조정

- 기여점
    - Biomedical multimodal instruction-following data: PMC-15를 기반으로 생물 의학 이미지에 대한 연구 결과의 전체 스펙트럼을 포괄하는 매우 다양한 시각 지침 추적 데이터셋을 생성
    - LLaVA-Med:  LLaVA를 생물 의학 도메인에 적응시키기 위한 새로운 커리큘럼 학습 방법을 제안.
        - 이미지-텍스트 쌍을 사용하여 생물 의학 용어에 맞게 LLaVA를 미세 조정
        - self-generated instruction following 데이터를 사용하여 모델을 계속 훈련하여 개방형 대화 의미를 학습
    - 오픈소스: 생물 의학 다중 모달 instruction following 데이터셋과 데이터 생성 및 모델 훈련을 위한 코드베이스

## 2. Related Work

- **Biomedical Chatbots**
    - 생물 의학 LLM 챗봇이 개발 (ChatDoctor, Med-Alpaca, PMC-LLaMA, Clinical Camel, DoctorGLM, Huatuo)
    - 오픈 소스 LLM으로 초기화되고 생물 의학 지침 추적 데이터의 맞춤 세트에서 미세 조정
    - 환자의 요구를 이해하고 정보에 입각한 조언을 제공하는 것과 같은 다양한 생물 의학 관련 분야/환경에서 도움을 줄 수 있는 큰 잠재력
    - Visual Med-Alpaca: 이미지 입력을 수용하는 유일한 다중 모달 생물 의학 챗봇
    - Visual Med-Alpaca와 LaVA-Med의 차이점
        - (i) 모델 아키텍처: LLaVA-Med는 엔드 투 엔드 신경 모델인 반면, Visual Med-Alpaca는 여러 이미지 캡셔닝 모델과 LLM을 연결하는 시스템으로 어떤 생물 의학 캡셔닝 모델이 이미지를 담당하는지를 결정하는 분류기를 사용
        - (ii) Biomedical instruction-following data: Visual Med-Alpaca는 제한된 생물 의학 주제 영역에서 54K 샘플로 훈련되었으나, LLaVA-Med는 보다 다양한 세트로 훈련

- **Biomedical Visual Question Answering**
    - 생물의학 이미지를 기반으로 질문에 답할 수 있는 모델
    - 판별적 방법과 생성적 방법
        - 판별적 방법:
            - VQA가 분류 문제, 사전 정의된 응답 집합으로부터 예측
            - 추론 단계에서 사용자 정의 답변 집합이 제공될 경우 추가가 필요
            - 다양한 질문에 답할 수 있는 일반-purpose 생물의학 보조기를 개발하는 목표에 대해 최적 X
            - → 생성적 방법 필요

- **Model Architecture**
    - LLaVA-Med는 언어 모델(LM)의 접두사 조정과 유사
        - 새로운 훈련 가능한 모듈이 고정된 이미지 인코더와 인과 LM을 연결
        - Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models [41]
    - [41]
        - 시각적 특징을 시각적 접두사로 매핑하기 위해 세 가지 층의 MLP 네트워크를 사용
        - 사전 훈련된 LM은 크기가 1.5B에서 2.7B까지 다양한 GPT2-XL [37], BioMedLM [42] 및 BioGPT [28]
        - 표준 감독 미세 조정만을 고려하고 다양한 모델링 선택을 탐구하는 데 집중
    - LLaVA-Med
        - 선형 투영 및 7B LM [43, 40]을 사용
        - 주요 기여: GPT-4를 사용하여 PubMed Central [49]에서 추출된 자유롭게 사용할 수 있는 광범위한 생물의학 이미지-텍스트 쌍을 활용

## 3. Biomedical Visual Instruction-Following Data

- 생물의학 멀티모달 instruction following 데이터 부족
- 두 가지 세트로 구성
    - concept alignment, instruction-following

- **Biomedical Concept Alignment Data**
    - 생물의학 이미지 $X_v$와 관련된 캡션 $X_c$에 대해, 우리는 생물의학 이미지를 설명하는 질문 $X_q$를 샘플링
        
        ![image](https://github.com/user-attachments/assets/0ef6efbf-d631-4857-81f7-fefeeb20d9b4)
        
    - 캡션의 길이에 따라 샘플링 된 질문은 이미지를 간결하게 설명하거나 자세하게 설명하도록 요청
    - PMC-15M [49]의 25% 캡션은 30 단어 미만 → 30 단어가 선택할 목록을 결정하기 위한 컷오프 포인트로 사용

- **Biomedical Instruction-Tuning Data**
    - 생물의학 이미지에 대한 다중 라운드 대화로 구성된 다양한 지침 따르기 데이터를 제시하고 큐레이션
        - 언어 전용 GPT-4를 통해 생성
        - 이미지 캡션이 주어지면, 우리는 GPT-4가 이미지를 볼 수 있었던 것처럼 다중 라운드 질문과 답변을 생성하도록 요청하는 instruction을 설계
        - 이미지 캡션이 짧아서 의미 있는 질문과 답변을 생성하기에 충분하지 않을 수 있음: 이미지를 보다 잘 이해하기 위해, 캡션 뿐만 아니라 이미지를 언급하는 원래의 PubMed 논문의 문장도 포함하는 프롬프트를 생성
        - 제공된 캡션과 맥락에 기반하여 고품질 대화를 생성하는 방법을 보여주기 위해 프롬프트 내에서 몇 가지 샘플 예제를 수작업으로 큐레이션
    - 다섯 가지 가장 일반적인 이미징 모달리티
        - CXR(흉부 X-선), CT(전산화 단층 촬영), MRI(자기 공명 영상), 조직병리학 및 육안병리학의 60K 이미지-텍스트 쌍을 샘플링
        - 캡션의 추가 맥락으로 원래 PubMed 논문에서 이미지를 언급하는 문장을 추출
        - 외부 지식이 일반화에 도움이 된다는 observations에 영감

![image 1](https://github.com/user-attachments/assets/32708ad7-1847-472a-a4d7-f092639bc7c3)

## 4. Adapting Multimodal Conversational Models to the Biomedical Domain

- LLaVA라는 일반 도메인 다중 모달 대화 모델을 초기 일반 도메인 언어 모델로 사용
    - 같은 네트워크 아키텍처를 사용하며, 선형 프로젝션 레이어가 비전 인코더와 언어 모델을 연결
- 생물 의학 도메인으로 지속적으로continuously 훈련

![image 2](https://github.com/user-attachments/assets/4e811b11-c95c-43bc-adef-3cc0f91168d2)

- Stage 1: Biomedical Concept Feature Alignment.
    - 개념 커버리지와 훈련 효율성 간의 균형을 맞추기 위해, PMC-15M에서 600K 이미지-텍스트 쌍으로 필터링
    - 각 샘플에 대해 언어 지침과 이미지 입력이 주어지면, 모델에게 원래의 캡션을 예측하도록
    - 훈련 중에는 비주얼 인코더와 언어 모델 가중치를 동결한 상태로 유지하고, 프로젝션 매트릭스만 업데이트

- Stage 2: End-to-End Instruction-Tuning
    - 시각적 인코더 가중치를 고정하고 프로젝션 레이어와 언어 모델의 사전 훈련된 가중치를 계속 업데이트
    - 섹션 3에서 수집된 생물 의학적 언어-이미지 지시 수행 데이터에서 모델을 미세 조정
    - 생물 의학적 VQA 데이터셋에서 평가할 때 뛰어난 제로샷 작업 전이 성능을 달

- Fine-tuning to Downstream Datasets
    - 특정 생물 의학적 시나리오에 따라, 보조 도우미의 서비스 품질을 개선하기 위해 매우 정확하고 데이터셋 특화된 모델을 개발해야 할 필요
    - 세 개의 생물 의학적 VQA 데이터셋에 대해 두 단계의 훈련 후 LLaVA-Med를 미세 조정
    - 여러 자연어 질문이 제공되며, 어시스턴트는 폐쇄형 및 개방형 질문에 대해 자유 형식의 텍스트로 응답
    - 폐쇄형 질문에 대한 답변 후보 목록은 프롬프트에 구성

- Discussion
    - favorable properties/implications of LLaVA-Med
    - (i) 중복 개발 비용
        - 저렴한 개발 비용으로 합리적인 솔루션을 제공하는 것을 목표
        - 8개의 40G A100 GPU를 사용했을 때 1단계와 2단계를 각각 7시간, 8시간 소요
    - (ii) A recipe for many domains 여러 도메인을 위한 조리법.
        - 생물 의학 분야에 초점을 맞추지만, 제안된 적응 절차는 게임 및 교육과 같은 다른 수직 분야에도 일반화 가능
    - (iii) 낮은 서비스 비용
        - 일반적인 LMM의 모델 크기가 거대할 수 있으며 서비스 비용이 지나치게 높을 수 있지만
        - 맞춤형 LMM은 낮은 서비스 비용이라는 고유한 장점
    - (iv) Smooth Model Adaptation
        - 네트워크 아키텍처는 비전 인코더를 BioMedCLIP [49]에서 초기화
        - 언어 모델을 Vicuna [43]에서 초기화

## 5. Experiments

- 두 가지 연구 평가 설정을 고려
    - (1) LLaVA-Med의 개방형 생물 의학 시각 챗봇으로서의 성능
    - (2) LLaVA-Med는 표준 벤치마크에서 기존 방법과 어떻게 비교되는가?

### 5.1 Biomedical Visual Chatbot

- 생물 의학 다중 모달 대화 성능을 평가하기 위해 193개의 새로운 질문으로 구성된 평가 데이터셋을 구축
    - PMC-15M에서 50개의 보지 못한 이미지와 캡션 쌍을 무작위로 선택
    - 대화와 상세 설명의 두 가지 유형의 질문을 생성

- GPT-4를 활용하여 이미지 맥락과 캡션이 주어졌을 때 질문에 대한 모델 응답의 정확성을 정량화
    - 다른 LMM에서 동일한 질문에 대한 응답을 생성
    - 두 어시스턴트의 응답에 대한 유용성, 관련성, 정확성 및 세부 수준을 점수화
    - 1에서 10까지의 전체 점수를 부여
    

![image 3](https://github.com/user-attachments/assets/ad20a6be-c2b7-413d-8997-10bd4d570f15)

- **결과**
    - Stage-1 훈련만으로는 LLaVA-Med가 챗봇으로서의 충분한 능력 X, 생물 의학 개념의 범위는 개선되지만 다양한 지시를 따르는 능력은 감소
    - 두 단계의 전체 교육을 받은 LLaVA-Med는 일반 도메인 LLaVA보다 일관되게 높은 성과
    - 더 큰 지시 데이터(10K에서 60K 샘플)로 훈련했을 때 더 높은 성과
    - 인라인 언급이 자기 지시(self-instruct)에서 고려될 때, 생성된 데이터 60K-IM는 채팅 능력을 약간 개선
    
    <aside>
    💡
    
    - **Inline mentions**:
        - "Inline mentions"는 문맥 속에서 언급된 특정 정보를 뜻합니다. 예를 들어, 텍스트 데이터에서 명시적으로 언급된 주요 키워드, 개념, 또는 대상이 이를 의미합니다.
        - 예를 들어, **"CT scan shows a mass in the left lung"**라는 문장이 주어진다면, "CT scan", "mass", "left lung" 같은 단어가 inline mention에 해당할 수 있습니다.
        - 이 개념은 데이터 생성 시 중요한 정보를 강조하거나 유지하기 위해 활용됩니다.
    - **Self-instruct**:
        - "Self-instruct"는 모델이 데이터를 학습하거나 생성할 때, 모델 자체적으로 또는 일정한 가이드라인을 통해 학습 데이터를 생성하거나 가공하는 방법론입니다.
        - 이는 보통 사람이 라벨링한 데이터가 부족할 때, 기존 데이터를 활용해 모델이 스스로 새로운 데이터를 생성하며 학습하도록 하는 전략입니다.
    </aside>
    
    - 전반적으로, 최상의 LLaVA-Med는 GPT-4의 50.2% 성과와 일치
    - GPT-4는 이미지 이해 없이도 실제 캡션 및 황금 인라인 언급을 고려하여 응답을 생성한다는 점에 유의
    - LLaVA-Med는 생물 의학 지식을 바탕으로 질문에 정확히 답변하는 반면, LLaVA는 상식에 기반하여 환각을 일으키는 일반인처럼 행동
    - 다중 모드 GPT-4는 공개되지 않았기 때문에, 비교를 위해 언어 전용 GPT-4를 사용

### 5.2 Performance on Established Benchmarks

![image 4](https://github.com/user-attachments/assets/53a4ae46-0519-4f1c-9c7b-961385dd694c)

- **Dataset Description**
    - 세 개의 생물 의학 VQA 데이터셋
    - VQA-RAD
        - 임상의가 생성한 3515 QA 쌍과 머리, 가슴, 복부에 고르게 분포된 315개의 방사선 이미지를 포함
        - 질문은 비정상, 속성, 양식, 장기 시스템, 색상, 계산, 객체/상태 존재, 크기, 평면, 위치적 추론 및 기타 11개 범주로 분류
        - 답변의 절반은 폐쇄형(예/아니오 유형)이며, 나머지는 한 단어 또는 짧은 구문으로 개방형
    - SLAKE
        - 의료 VQA를 위한 의미적으로 레이블이 지정된 지식 강화 데이터 세트
        - 642개의 방사선 이미지를 포함하고, 7000개 이상의 다양한 QA 쌍이 경험이 풍부한 의사들에 의해 주석 처리
        - 질문은 외부 의료 지식을 포함할 수 있으며(제공된 의료 지식 그래프에 의해 해결됨)
        - 이미지는 의미적 분할 마스크와 객체 탐지 경계 상자를 포함한 풍부한 시각적 주석이 연결
        - SLAKE는 현재 이용 가능한 데이터 세트보다 더 많은 인체 부위를 포괄하며, 뇌, 목, 가슴, 복부 및 골반을 포함한 더 풍부한 양식을 제공
        - 영어와 중국어를 포함한 이중 언어 데이터 세트 → 영어만 사용
    - PathVQA
        - 병리학 이미지를 위한 데이터 세트, 총 4998개의 병리학 이미지를 포함하고 32,799개의 QA 쌍
        - 각 이미지는 위치, 형태, 색상, 외관 등의 여러 측면과 관련된 여러 질문
        - 개방형 질문과 폐쇄형 질문 두 가지 유형으로 분류되며 여러 가지 변형

- **Evaluation Metrics**
    - 폐쇄형 질문의 경우 정확도
    - 개방형 질문의 경우, 생성된 시퀀스에서 정답 토큰이 나타나는 비율을 평가하기 위해 재현율을 사용

- **Comparisons with SoTA**
    - LLaVA-Med를 일반 도메인 LLaVA 및 기존 대표 방법들과 비교
    - 모든 LLaVA-Med 변형이 LLaVA를 능가
        - LLaVA 또는 Vicuna와의 언어 모델 초기화의 차이는 미미하지만, BioMed CLIP에서 시각 인코더의 초기화는 일반 도메인 CLIP보다 약간 더 우수
    - LLaVA-Med의 미세 조정 성능이 VQA-RAD 및 PathVQA의 폐쇄형 질문에 대한 감독된 최신 기술보다 높습니다
        - 명확한 지침이 제공될 때 LLaVA-Med가 생물의학 작업을 수행하는 데 강력한 능력을 가지고 있음을 검증
    - 개방형 질문에 대해 LLaVA-Med는 SLAKE에서 최신 기술을 달성, 다른 데이터 세트에서는 성능이 제한적
        - 개방형 생물의학 질문이 기대되는 답변 옵션을 제한하지 않으면 모호할 수 있기 때문으로 추측

- **Ablation Studies**
    - (i) LLaVA-Med는 LLaVA를 큰 차이로 지속적으로 능가, 생물의학 도메인 특정 적응의 효과성
    - (ii) 1단계에서 더 오랜 훈련은 제로샷 전이를 개선하지만, 1단계만으로는 충분 X
        - 1단계에서의 단일 이미지 캡션 지침이 모델이 다양한 지침을 따르는 능력을 잃게 할 수 있기 때문
    - (iii) 2단계에서의 지침 따르기 데이터는 중요하며, 지침 데이터 양이 10K에서 60K로 증가하면서 성능이 일반적으로 향상
        - 60K-IM 데이터는 평균 제로샷 및 미세 조정 성능을 각각 가장 잘 제공
        - 인라인 언급을 외부 지식으로 고려하는 것의 효과성을 검증 (주어진 텍스트 안에 명시적으로 포함된 정보를 외부 지식처럼 중요한 정보로 간주하여 학습에 반영)
    - (iv) 다운스트림 데이터 세트에서 9 epoch까지 더 긴 미세 조정은 성능에 이점
        - 특히 2단계에서 3 epoch 훈련이 이루어진 체크포인트
        - 언어 모델 크기를 7B에서 13B로 늘리면 전체 제로샷 성능과 미세 조정 성능이 향상

![image 5](https://github.com/user-attachments/assets/251a668e-0fd4-492b-afa3-54eef1953520)

- **Case Study I: Zero-shot on Chinese Questions**
    - 60K-IM 데이터에서 훈련된 LLaVA-Med에 대해, SLAKE 데이터 세트에서 중국어 질문을 제공
    - LLaVA-Med의 훈련에는 중국어 지침을 따르는 데이터가 포함X, LLaVA-Med가 중국어 질문을 정확하게 이해하고 올바른 답변을 제공 가능
    - LLaMA/Vicuna에서 습득한 다국어 지식 덕분

## 6. Conclusions

- 생물 의학 분야를 위한 대규모 언어 및 비전 모델인 LLaVA-Med
- 생물 의학 언어-이미지 instruction following 데이터 세트 생성
- 도메인 지식과 함께 강력한 채팅 능력을 입증, 특정 지표에서 세 가지 VQA 데이터 세트에서 이전의 감독된 SoTA를 능가
- LLaVA-Med가 많은 대형 언어 모델에서 일반적으로 나타나는 환각(hallucination)과 깊이 있는 추론이 약하다는 한계를 지적